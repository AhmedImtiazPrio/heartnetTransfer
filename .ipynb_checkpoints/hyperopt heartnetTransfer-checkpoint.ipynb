{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from keras.constraints import max_norm\n",
    "from keras.regularizers import l2\n",
    "from custom_layers import Conv1D_linearphase\n",
    "from heartnet_v1 import heartnet, reshape_folds, branch\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "set_session(tf.Session(config=config))\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, TensorBoard, Callback\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Dropout, Concatenate, initializers, Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "import tables\n",
    "from datetime import datetime\n",
    "from hyperopt import hp, fmin, Trials, tpe, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weight(Y, classes):\n",
    "    num_samples = (len(Y))\n",
    "    n_classes = (len(classes))\n",
    "    num_bin = np.sum(Y,axis=-2)\n",
    "    class_weights = {i: (num_samples / (n_classes * num_bin[i])) for i in range(n_classes)}\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heartnet_transfer(load_path='/media/taufiq/Data1/heart_sound/weights.0148-0.8902.hdf5',\n",
    "                      lr=0.0012843784,lr_decay=0.0001132885,\n",
    "                      num_dense1=20,num_dense2=20,trainable=False,dropout_rate=0.):\n",
    "    model = heartnet(load_path=load_path,FIR_train=False,trainable=trainable)\n",
    "    plot_model(model,'before.png',show_shapes=True,show_layer_names=True)\n",
    "    x = model.layers[-4].output\n",
    "    x = Dense(num_dense1,activation='relu',kernel_initializer=initializers.he_uniform(seed=1)) (x)\n",
    "    x = Dropout(rate=dropout_rate,seed=1) (x)\n",
    "    x = Dense(num_dense2, activation='relu',kernel_initializer=initializers.he_normal(seed=1))(x)\n",
    "    x = Dropout(rate=dropout_rate, seed=1)(x)\n",
    "    output = Dense(3,activation='softmax')(x)\n",
    "    model = Model(inputs=model.input,outputs=output)\n",
    "    plot_model(model, 'after.png',show_shapes=True,show_layer_names=True)\n",
    "    if load_path:\n",
    "        model.load_weights(load_path,by_name=True)\n",
    "    sgd = Adam(lr=lr)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heartnet_transfer(load_path='/media/taufiq/Data1/heart_sound/weights.0148-0.8902.hdf5',\n",
    "                      lr=0.0012843784,lr_decay=0.0001132885,\n",
    "                      num_dense1=20,num_dense2=20,trainable=False,dropout_rate=0.):\n",
    "    model = heartnet(load_path=load_path,FIR_train=False,trainable=trainable)\n",
    "    plot_model(model,'before.png',show_shapes=True,show_layer_names=True)\n",
    "#     x = model.layers[-4].output\n",
    "#     x = Dense(num_dense1,activation='relu',kernel_initializer=initializers.he_uniform(seed=1)) (x)\n",
    "#     x = Dropout(rate=dropout_rate,seed=1) (x)\n",
    "#     x = Dense(num_dense2, activation='relu',kernel_initializer=initializers.he_normal(seed=1))(x)\n",
    "#     x = Dropout(rate=dropout_rate, seed=1)(x)\n",
    "#     output = Dense(3,activation='softmax')(x)\n",
    "#     model = Model(inputs=model.input,outputs=output)\n",
    "#     plot_model(model, 'after.png',show_shapes=True,show_layer_names=True)\n",
    "    if load_path:\n",
    "        model.load_weights(load_path,by_name=True)\n",
    "    sgd = Adam(lr=lr)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_recall(Callback):\n",
    "\n",
    "    def __init__(self, x_val, y_val, val_parts):\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.val_parts = val_parts\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if logs is not None:\n",
    "            y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "            y_pred = np.argmax(y_pred, axis=-1)\n",
    "            self.y_val_ = np.transpose(np.argmax(self.y_val, axis=-1))\n",
    "            true = []\n",
    "            pred = []\n",
    "            start_idx = 0\n",
    "            for s in self.val_parts:\n",
    "\n",
    "                if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "                    continue\n",
    "                # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "\n",
    "                temp_ = self.y_val_[start_idx:start_idx + int(s) - 1]\n",
    "                temp = y_pred[start_idx:start_idx + int(s) - 1]\n",
    "\n",
    "                if (sum(temp == 0) > sum(temp == 1)) and (sum(temp == 0) > sum(temp == 2)):\n",
    "                    pred.append(0)\n",
    "                elif (sum(temp == 2) > sum(temp == 1)) and (sum(temp == 2) > sum(temp == 0)):\n",
    "                    pred.append(2)\n",
    "                else:\n",
    "                    pred.append(1)\n",
    "\n",
    "                if (sum(temp_ == 0) > sum(temp_ == 1)) and (sum(temp_ == 0) > sum(temp_ == 2)):\n",
    "                    true.append(0)\n",
    "                elif (sum(temp_ == 2) > sum(temp_ == 1)) and (sum(temp_ == 2) > sum(temp_ == 0)):\n",
    "                    true.append(2)\n",
    "                else:\n",
    "                    true.append(1)\n",
    "\n",
    "                start_idx = start_idx + int(s)\n",
    "\n",
    "            confmat = confusion_matrix(y_pred=pred, y_true=true)\n",
    "            \n",
    "            logs['recall0'] = confmat[0,0]/np.sum(confmat[0,:])\n",
    "            logs['recall1'] = confmat[1,1]/np.sum(confmat[1,:])\n",
    "            logs['recall2'] = confmat[2,2]/np.sum(confmat[2,:])\n",
    "            logs['UAR'] = np.mean([logs['recall0'],logs['recall1'],logs['recall2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_dir = '/media/taufiq/Data1/heart_sound/feature/segmented_noFIR/'\n",
    "foldname = 'comParE'\n",
    "model_dir = '/media/taufiq/Data1/heart_sound/models/'\n",
    "log_dir = '/media/taufiq/Data1/heart_sound/logs/'\n",
    "\n",
    "##### Load Model ######\n",
    "load_path='/media/taufiq/Data1/heart_sound/weights.0169-0.8798.hdf5'\n",
    "# lr = 0.00001\n",
    "lr = 1e-6\n",
    "num_dense1 = 239 #34,120,167,239,1239,650,788,422,598\n",
    "num_dense2 = 137 #121,\n",
    "epochs = 40\n",
    "batch_size = 256\n",
    "dropout_rate = 0.\n",
    "trainable = False\n",
    "addweights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17061, 2500, 1)\n",
      "(17061, 1)\n",
      "(5872, 2500, 1)\n",
      "(5872, 1)\n"
     ]
    }
   ],
   "source": [
    "feat = tables.open_file(fold_dir + foldname + '.mat')\n",
    "x_train = feat.root.trainX[:]\n",
    "y_train = feat.root.trainY[0, :]\n",
    "x_val = feat.root.valX[:]\n",
    "y_val = feat.root.valY[0, :]\n",
    "train_parts = feat.root.train_parts[:]\n",
    "val_parts = feat.root.val_parts[0, :]\n",
    "############### Reshaping ############\n",
    "x_train, y_train, x_val, y_val = reshape_folds(x_train, x_val, y_train, y_val)\n",
    "# y_train = to_categorical(y_train,num_classes=3)\n",
    "# y_val = to_categorical(y_val,num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1134., 4738.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum((y_val),axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## relabel to 2 class\n",
    "for idx in range(x_val.shape[0]):\n",
    "    if y_val[idx,0] == 2:\n",
    "        y_val[idx,0]=1\n",
    "y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = heartnet_transfer(load_path=load_path,lr=lr,num_dense1=num_dense1,\n",
    "                              num_dense2=num_dense2,trainable=trainable,\n",
    "                              dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5872/5872 [==============================] - 1s 133us/step\n",
      "[ 55. 125.]\n",
      "[[ 10  22]\n",
      " [ 45 103]]\n",
      "1.00844594595\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8aa1b45df093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mconfmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m32\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m103\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m148\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "plot_model(model,\"model.png\",show_layer_names=True,show_shapes=True)\n",
    "y_pred = model.predict(x_val, verbose=1)\n",
    "y_pred = np.argmax(y_pred, axis=-1)\n",
    "y_val_ = np.transpose(np.argmax(y_val, axis=-1))\n",
    "true = []\n",
    "pred = []\n",
    "start_idx = 0\n",
    "for s in val_parts:\n",
    "\n",
    "    if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "        continue\n",
    "    # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "\n",
    "    temp_ = y_val_[start_idx:start_idx + int(s) - 1]\n",
    "    temp = y_pred[start_idx:start_idx + int(s) - 1]\n",
    "\n",
    "    if (sum(temp == 0) > sum(temp == 1)) and (sum(temp == 0) > sum(temp == 2)):\n",
    "        pred.append(0)\n",
    "    elif (sum(temp == 2) > sum(temp == 1)) and (sum(temp == 2) > sum(temp == 0)):\n",
    "        pred.append(2)\n",
    "    else:\n",
    "        pred.append(1)\n",
    "\n",
    "    if (sum(temp_ == 0) > sum(temp_ == 1)) and (sum(temp_ == 0) > sum(temp_ == 2)):\n",
    "        true.append(0)\n",
    "    elif (sum(temp_ == 2) > sum(temp_ == 1)) and (sum(temp_ == 2) > sum(temp_ == 0)):\n",
    "        true.append(2)\n",
    "    else:\n",
    "        true.append(1)\n",
    "\n",
    "    start_idx = start_idx + int(s)\n",
    "\n",
    "print(np.sum(to_categorical(pred),axis=-2))\n",
    "confmat = confusion_matrix(y_pred=pred, y_true=true)\n",
    "print(confmat)\n",
    "print((10/32 + 103/148)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(args):\n",
    "    #### Parse arguments and print #####\n",
    "    \n",
    "    print(\"args %s\" % args)\n",
    "    \n",
    "#     lr = args['lr']\n",
    "#     num_dense1 = args['num_dense1']\n",
    "    num_dense2 = args['num_dense2']\n",
    "    \n",
    "    ##### Load model ######\n",
    "    model = heartnet_transfer(load_path=load_path,lr=lr,num_dense1=num_dense1,\n",
    "                              num_dense2=num_dense2,trainable=trainable,\n",
    "                              dropout_rate=dropout_rate)\n",
    "    \n",
    "    #### Log params #####\n",
    "    log_name = foldname + ' ' + str(datetime.now()) + str(args.values())\n",
    "    checkpoint_name = model_dir + log_name + \"/\" + 'weights.{epoch:04d}-{val_acc:.4f}.hdf5'\n",
    "    if not os.path.exists(model_dir + log_name):\n",
    "        os.makedirs(model_dir + log_name)\n",
    "    plot_model(model,\"model.png\",show_layer_names=True,show_shapes=True)\n",
    "    \n",
    "    ### Callbacks ###\n",
    "    \n",
    "    csv_logger = CSVLogger(log_dir + log_name + '/training.csv')\n",
    "    modelcheckpnt = ModelCheckpoint(filepath=checkpoint_name,\n",
    "                                    monitor='val_acc', save_best_only=False, mode='max')\n",
    "    tensbd = TensorBoard(log_dir=log_dir + log_name,\n",
    "                         batch_size=batch_size,\n",
    "                         # histogram_freq=100,\n",
    "                         # embeddings_freq=99,\n",
    "                         # embeddings_layer_names=embedding_layer_names,\n",
    "                         # embeddings_data=x_val,\n",
    "                         # embeddings_metadata=metadata_file,\n",
    "                         write_images=False)\n",
    "    class_weights=compute_weight(y_train,range(3))\n",
    "    print(\"Class weights %s\" % class_weights)\n",
    "\n",
    "    #### Train ####\n",
    "    \n",
    "    history = model.fit(x_train,y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=2,\n",
    "                        shuffle=True,\n",
    "                        class_weight=class_weights,\n",
    "                        callbacks=[modelcheckpnt,\n",
    "                        log_recall(x_val, y_val, val_parts),\n",
    "                        tensbd, csv_logger],\n",
    "                        validation_data=(x_val,y_val))\n",
    "#     print(\"History : %s\" % history.history)\n",
    "    loss = history.history['val_loss']\n",
    "    print(loss)\n",
    "    K.clear_session()\n",
    "#     return (1.- np.float32(np.max(loss)))\n",
    "    return {'loss': np.min(loss), 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "#     'lr' : 10 ** hp.uniform('lr',-8,-3),\n",
    "#     'num_dense1' : 200 + hp.randint('num_dense1',1800),\n",
    "    'num_dense2' : 40 + hp.randint('num_dense2',360)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args {'num_dense2': 89}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 5s - loss: 1.2428 - acc: 0.3390 - val_loss: 1.1510 - val_acc: 0.3442\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.2202 - acc: 0.3444 - val_loss: 1.1484 - val_acc: 0.3360\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2046 - acc: 0.3426 - val_loss: 1.1463 - val_acc: 0.3399\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.1864 - acc: 0.3595 - val_loss: 1.1439 - val_acc: 0.3408\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.1795 - acc: 0.3653 - val_loss: 1.1423 - val_acc: 0.3394\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.1674 - acc: 0.3720 - val_loss: 1.1429 - val_acc: 0.3374\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.1539 - acc: 0.3764 - val_loss: 1.1410 - val_acc: 0.3377\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1493 - acc: 0.3775 - val_loss: 1.1421 - val_acc: 0.3338\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1350 - acc: 0.3887 - val_loss: 1.1406 - val_acc: 0.3374\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1342 - acc: 0.3859 - val_loss: 1.1411 - val_acc: 0.3362\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1255 - acc: 0.3905 - val_loss: 1.1405 - val_acc: 0.3370\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1145 - acc: 0.3984 - val_loss: 1.1394 - val_acc: 0.3406\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1171 - acc: 0.4033 - val_loss: 1.1395 - val_acc: 0.3392\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1053 - acc: 0.4056 - val_loss: 1.1391 - val_acc: 0.3382\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1069 - acc: 0.4089 - val_loss: 1.1401 - val_acc: 0.3334\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.0988 - acc: 0.4118 - val_loss: 1.1379 - val_acc: 0.3353\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.0917 - acc: 0.4194 - val_loss: 1.1390 - val_acc: 0.3351\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.0871 - acc: 0.4232 - val_loss: 1.1409 - val_acc: 0.3324\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.0896 - acc: 0.4296 - val_loss: 1.1395 - val_acc: 0.3340\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.0769 - acc: 0.4295 - val_loss: 1.1383 - val_acc: 0.3391\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0764 - acc: 0.4290 - val_loss: 1.1379 - val_acc: 0.3401\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0791 - acc: 0.4317 - val_loss: 1.1399 - val_acc: 0.3362\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0683 - acc: 0.4361 - val_loss: 1.1388 - val_acc: 0.3397\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0664 - acc: 0.4364 - val_loss: 1.1387 - val_acc: 0.3411\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0657 - acc: 0.4445 - val_loss: 1.1390 - val_acc: 0.3426\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0583 - acc: 0.4451 - val_loss: 1.1375 - val_acc: 0.3437\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0612 - acc: 0.4470 - val_loss: 1.1395 - val_acc: 0.3432\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0560 - acc: 0.4450 - val_loss: 1.1393 - val_acc: 0.3443\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0460 - acc: 0.4567 - val_loss: 1.1389 - val_acc: 0.3442\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0497 - acc: 0.4562 - val_loss: 1.1401 - val_acc: 0.3442\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0468 - acc: 0.4545 - val_loss: 1.1407 - val_acc: 0.3428\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0417 - acc: 0.4612 - val_loss: 1.1410 - val_acc: 0.3437\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0415 - acc: 0.4586 - val_loss: 1.1428 - val_acc: 0.3416\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0376 - acc: 0.4599 - val_loss: 1.1411 - val_acc: 0.3445\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0369 - acc: 0.4629 - val_loss: 1.1418 - val_acc: 0.3471\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0297 - acc: 0.4673 - val_loss: 1.1420 - val_acc: 0.3476\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0313 - acc: 0.4655 - val_loss: 1.1440 - val_acc: 0.3443\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0318 - acc: 0.4640 - val_loss: 1.1436 - val_acc: 0.3484\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0291 - acc: 0.4686 - val_loss: 1.1448 - val_acc: 0.3452\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0234 - acc: 0.4721 - val_loss: 1.1436 - val_acc: 0.3515\n",
      "[1.1510249142425912, 1.1484084743245095, 1.1462839666439337, 1.1439054025291422, 1.1422652334218455, 1.1428651575821298, 1.1409935649149425, 1.1421159859898955, 1.1406490166115826, 1.1411008360600277, 1.140519400058715, 1.139388303665764, 1.1395345975007933, 1.1391345681546494, 1.1400686622640417, 1.137946776538194, 1.1389717246920925, 1.1409102522384893, 1.1395261670977932, 1.1382934098672477, 1.1378522984663213, 1.1398889446778258, 1.13878983925084, 1.1387482501505506, 1.1389983236302472, 1.1375380345196424, 1.139520861147535, 1.1392583408537613, 1.1389330294216686, 1.1401270079028054, 1.14072147896894, 1.1410336111157078, 1.1428024996204011, 1.1410767986599042, 1.1418093764489614, 1.1420404472532975, 1.1439593650339734, 1.1436253792583455, 1.1447597371784803, 1.143590560401168]\n",
      "args {'num_dense2': 375}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.5886 - acc: 0.3794 - val_loss: 1.1296 - val_acc: 0.3878\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3883 - acc: 0.3856 - val_loss: 1.1259 - val_acc: 0.3694\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.3165 - acc: 0.3643 - val_loss: 1.1391 - val_acc: 0.3375\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2875 - acc: 0.3601 - val_loss: 1.1448 - val_acc: 0.3215\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2661 - acc: 0.3592 - val_loss: 1.1459 - val_acc: 0.3193\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2448 - acc: 0.3643 - val_loss: 1.1457 - val_acc: 0.3176\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2286 - acc: 0.3640 - val_loss: 1.1429 - val_acc: 0.3212\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.2233 - acc: 0.3672 - val_loss: 1.1416 - val_acc: 0.3229\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.2133 - acc: 0.3762 - val_loss: 1.1416 - val_acc: 0.3224\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1962 - acc: 0.3762 - val_loss: 1.1419 - val_acc: 0.3222\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1880 - acc: 0.3824 - val_loss: 1.1406 - val_acc: 0.3270\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1741 - acc: 0.3893 - val_loss: 1.1400 - val_acc: 0.3290\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1677 - acc: 0.3936 - val_loss: 1.1427 - val_acc: 0.3282\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1647 - acc: 0.3958 - val_loss: 1.1423 - val_acc: 0.3295\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1467 - acc: 0.3960 - val_loss: 1.1433 - val_acc: 0.3304\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1459 - acc: 0.4009 - val_loss: 1.1426 - val_acc: 0.3348\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1382 - acc: 0.4062 - val_loss: 1.1434 - val_acc: 0.3357\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1335 - acc: 0.4091 - val_loss: 1.1414 - val_acc: 0.3411\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1200 - acc: 0.4178 - val_loss: 1.1431 - val_acc: 0.3404\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1178 - acc: 0.4190 - val_loss: 1.1447 - val_acc: 0.3386\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1144 - acc: 0.4168 - val_loss: 1.1459 - val_acc: 0.3386\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.1115 - acc: 0.4237 - val_loss: 1.1453 - val_acc: 0.3403\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0952 - acc: 0.4304 - val_loss: 1.1454 - val_acc: 0.3426\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0982 - acc: 0.4250 - val_loss: 1.1484 - val_acc: 0.3423\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0947 - acc: 0.4301 - val_loss: 1.1477 - val_acc: 0.3437\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0850 - acc: 0.4333 - val_loss: 1.1469 - val_acc: 0.3462\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0827 - acc: 0.4391 - val_loss: 1.1476 - val_acc: 0.3443\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0747 - acc: 0.4442 - val_loss: 1.1479 - val_acc: 0.3462\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0711 - acc: 0.4428 - val_loss: 1.1489 - val_acc: 0.3466\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0715 - acc: 0.4391 - val_loss: 1.1513 - val_acc: 0.3467\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0713 - acc: 0.4445 - val_loss: 1.1519 - val_acc: 0.3481\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0619 - acc: 0.4501 - val_loss: 1.1514 - val_acc: 0.3493\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0574 - acc: 0.4538 - val_loss: 1.1512 - val_acc: 0.3529\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0578 - acc: 0.4530 - val_loss: 1.1521 - val_acc: 0.3508\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0462 - acc: 0.4569 - val_loss: 1.1524 - val_acc: 0.3527\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0498 - acc: 0.4568 - val_loss: 1.1540 - val_acc: 0.3517\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0447 - acc: 0.4633 - val_loss: 1.1547 - val_acc: 0.3503\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0369 - acc: 0.4620 - val_loss: 1.1564 - val_acc: 0.3489\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0425 - acc: 0.4639 - val_loss: 1.1569 - val_acc: 0.3484\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0366 - acc: 0.4655 - val_loss: 1.1559 - val_acc: 0.3484\n",
      "[1.1296056021786516, 1.1259279303069984, 1.1391331558331481, 1.1447810012573116, 1.1458506528947918, 1.1456985044869155, 1.142943645692976, 1.1415830174973616, 1.1416189394464935, 1.1418649932669034, 1.1406315350727425, 1.1399624000128348, 1.1426590499176317, 1.142276132464084, 1.1433112829192782, 1.1426033382519714, 1.1433527492372477, 1.1414256021177411, 1.1431337442320115, 1.1447369850948657, 1.1459098362467919, 1.1453121075513253, 1.1453657913597792, 1.1483731874003398, 1.147658314302117, 1.1468529756452472, 1.1475707094415981, 1.1478572463469545, 1.1488912836407446, 1.1513187729370367, 1.151924196316046, 1.1513749045961885, 1.1512186952767645, 1.1521034805911115, 1.1523620185800079, 1.1539544757120617, 1.1547260492309237, 1.1564290617727129, 1.1568690522165324, 1.1558702121963293]\n",
      "args {'num_dense2': 149}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.3729 - acc: 0.3352 - val_loss: 1.1903 - val_acc: 0.2815\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3121 - acc: 0.3320 - val_loss: 1.1896 - val_acc: 0.2679\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2826 - acc: 0.3389 - val_loss: 1.1802 - val_acc: 0.2778\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2723 - acc: 0.3369 - val_loss: 1.1766 - val_acc: 0.2822\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2425 - acc: 0.3523 - val_loss: 1.1711 - val_acc: 0.2885\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2321 - acc: 0.3577 - val_loss: 1.1676 - val_acc: 0.2927\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2228 - acc: 0.3706 - val_loss: 1.1659 - val_acc: 0.2924\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.2016 - acc: 0.3713 - val_loss: 1.1642 - val_acc: 0.2950\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1983 - acc: 0.3724 - val_loss: 1.1614 - val_acc: 0.3026\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1803 - acc: 0.3855 - val_loss: 1.1622 - val_acc: 0.3016\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1726 - acc: 0.3843 - val_loss: 1.1612 - val_acc: 0.3043\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1618 - acc: 0.3874 - val_loss: 1.1600 - val_acc: 0.3072\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1526 - acc: 0.3925 - val_loss: 1.1612 - val_acc: 0.3060\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1464 - acc: 0.3994 - val_loss: 1.1609 - val_acc: 0.3050\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1336 - acc: 0.3960 - val_loss: 1.1608 - val_acc: 0.3053\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1408 - acc: 0.3970 - val_loss: 1.1616 - val_acc: 0.3081\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1255 - acc: 0.4014 - val_loss: 1.1609 - val_acc: 0.3134\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1184 - acc: 0.4045 - val_loss: 1.1598 - val_acc: 0.3176\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1178 - acc: 0.4121 - val_loss: 1.1599 - val_acc: 0.3169\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1073 - acc: 0.4207 - val_loss: 1.1594 - val_acc: 0.3186\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1066 - acc: 0.4155 - val_loss: 1.1601 - val_acc: 0.3193\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.1013 - acc: 0.4238 - val_loss: 1.1597 - val_acc: 0.3208\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0936 - acc: 0.4203 - val_loss: 1.1590 - val_acc: 0.3225\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0940 - acc: 0.4211 - val_loss: 1.1588 - val_acc: 0.3244\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0893 - acc: 0.4306 - val_loss: 1.1609 - val_acc: 0.3224\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0842 - acc: 0.4221 - val_loss: 1.1608 - val_acc: 0.3231\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0794 - acc: 0.4296 - val_loss: 1.1625 - val_acc: 0.3237\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0770 - acc: 0.4320 - val_loss: 1.1621 - val_acc: 0.3224\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0677 - acc: 0.4358 - val_loss: 1.1616 - val_acc: 0.3254\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0620 - acc: 0.4398 - val_loss: 1.1622 - val_acc: 0.3244\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0604 - acc: 0.4386 - val_loss: 1.1638 - val_acc: 0.3214\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0595 - acc: 0.4444 - val_loss: 1.1635 - val_acc: 0.3215\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0498 - acc: 0.4493 - val_loss: 1.1634 - val_acc: 0.3243\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0548 - acc: 0.4487 - val_loss: 1.1635 - val_acc: 0.3234\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0424 - acc: 0.4524 - val_loss: 1.1632 - val_acc: 0.3234\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0396 - acc: 0.4580 - val_loss: 1.1639 - val_acc: 0.3229\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0441 - acc: 0.4581 - val_loss: 1.1655 - val_acc: 0.3231\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0371 - acc: 0.4595 - val_loss: 1.1646 - val_acc: 0.3246\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0417 - acc: 0.4615 - val_loss: 1.1646 - val_acc: 0.3248\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0275 - acc: 0.4622 - val_loss: 1.1636 - val_acc: 0.3246\n",
      "[1.1903033867194153, 1.1896417972502331, 1.180213819732458, 1.1765557176735486, 1.171116098720956, 1.1675701722786926, 1.1659096960150903, 1.164183381467814, 1.1613979579967115, 1.1622456701964707, 1.1612230042345841, 1.1599744415413131, 1.1612079003851161, 1.1609438079579324, 1.1607714780994591, 1.1615526309130302, 1.1608715560845524, 1.1597832054793022, 1.1599296721190782, 1.1594024058583647, 1.1601134343757942, 1.1596603338335125, 1.1590134993560957, 1.1588064711490185, 1.1609088072010216, 1.1608048066781067, 1.162484979109803, 1.162120598863191, 1.1616360368130967, 1.1622364267666268, 1.163763104082778, 1.163476032522134, 1.1633988456440232, 1.1635353610690997, 1.1632178557341366, 1.163892497808472, 1.1654663735579405, 1.1646169021279025, 1.1646401268260032, 1.1635656187904628]\n",
      "args {'num_dense2': 101}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.7696 - acc: 0.1913 - val_loss: 1.3468 - val_acc: 0.2134\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.4415 - acc: 0.2479 - val_loss: 1.2163 - val_acc: 0.3016\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.3366 - acc: 0.3071 - val_loss: 1.1682 - val_acc: 0.3520\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2960 - acc: 0.3433 - val_loss: 1.1496 - val_acc: 0.3709\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2897 - acc: 0.3559 - val_loss: 1.1405 - val_acc: 0.3787\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2619 - acc: 0.3672 - val_loss: 1.1354 - val_acc: 0.3786\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2438 - acc: 0.3751 - val_loss: 1.1316 - val_acc: 0.3789\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.2321 - acc: 0.3760 - val_loss: 1.1288 - val_acc: 0.3782\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.2185 - acc: 0.3805 - val_loss: 1.1252 - val_acc: 0.3798\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.2132 - acc: 0.3843 - val_loss: 1.1234 - val_acc: 0.3793\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1969 - acc: 0.3830 - val_loss: 1.1223 - val_acc: 0.3776\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1841 - acc: 0.3933 - val_loss: 1.1217 - val_acc: 0.3774\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1660 - acc: 0.3971 - val_loss: 1.1192 - val_acc: 0.3798\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1616 - acc: 0.4026 - val_loss: 1.1197 - val_acc: 0.3753\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1483 - acc: 0.4107 - val_loss: 1.1183 - val_acc: 0.3738\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1503 - acc: 0.4015 - val_loss: 1.1186 - val_acc: 0.3760\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1384 - acc: 0.4072 - val_loss: 1.1187 - val_acc: 0.3748\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1263 - acc: 0.4134 - val_loss: 1.1186 - val_acc: 0.3743\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1249 - acc: 0.4169 - val_loss: 1.1187 - val_acc: 0.3767\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1174 - acc: 0.4205 - val_loss: 1.1181 - val_acc: 0.3740\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1060 - acc: 0.4281 - val_loss: 1.1186 - val_acc: 0.3741\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.1060 - acc: 0.4265 - val_loss: 1.1185 - val_acc: 0.3718\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.1064 - acc: 0.4220 - val_loss: 1.1191 - val_acc: 0.3706\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0994 - acc: 0.4270 - val_loss: 1.1193 - val_acc: 0.3697\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0949 - acc: 0.4283 - val_loss: 1.1196 - val_acc: 0.3690\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0870 - acc: 0.4354 - val_loss: 1.1193 - val_acc: 0.3704\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0856 - acc: 0.4375 - val_loss: 1.1208 - val_acc: 0.3687\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0766 - acc: 0.4382 - val_loss: 1.1214 - val_acc: 0.3684\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0775 - acc: 0.4419 - val_loss: 1.1222 - val_acc: 0.3704\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0737 - acc: 0.4402 - val_loss: 1.1229 - val_acc: 0.3680\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0671 - acc: 0.4449 - val_loss: 1.1225 - val_acc: 0.3697\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0634 - acc: 0.4506 - val_loss: 1.1220 - val_acc: 0.3697\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0567 - acc: 0.4473 - val_loss: 1.1235 - val_acc: 0.3678\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0483 - acc: 0.4605 - val_loss: 1.1243 - val_acc: 0.3685\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0534 - acc: 0.4542 - val_loss: 1.1240 - val_acc: 0.3680\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0554 - acc: 0.4506 - val_loss: 1.1242 - val_acc: 0.3699\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0517 - acc: 0.4598 - val_loss: 1.1261 - val_acc: 0.3675\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0441 - acc: 0.4585 - val_loss: 1.1260 - val_acc: 0.3689\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0398 - acc: 0.4584 - val_loss: 1.1258 - val_acc: 0.3702\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0413 - acc: 0.4626 - val_loss: 1.1265 - val_acc: 0.3706\n",
      "[1.346757898538574, 1.2163061228370147, 1.168228589546453, 1.1496100094402844, 1.1404951339846412, 1.135407306842648, 1.1315686520828538, 1.1288332156329455, 1.1251823976838946, 1.1233812372431118, 1.122298378385705, 1.1216901556997274, 1.1191656053553485, 1.1197362452826642, 1.1182848728320254, 1.1186276654781373, 1.1187166459553899, 1.1185780770122518, 1.1187111129552856, 1.118107222731172, 1.1185645319785344, 1.1185401898963574, 1.119144300673898, 1.1193051994334124, 1.119563547077231, 1.1192836433405446, 1.120757133174657, 1.121382194253989, 1.1222460282920816, 1.1229231110710538, 1.1224825798003486, 1.121992737785672, 1.1234826651840835, 1.1243079816288128, 1.1240482599923656, 1.124246222121839, 1.1260688691438057, 1.1260178404218169, 1.1258260346238556, 1.126508028045987]\n",
      "args {'num_dense2': 292}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.3332 - acc: 0.3295 - val_loss: 1.1653 - val_acc: 0.3261\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.2918 - acc: 0.3303 - val_loss: 1.1609 - val_acc: 0.3372\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2730 - acc: 0.3421 - val_loss: 1.1578 - val_acc: 0.3421\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2470 - acc: 0.3568 - val_loss: 1.1549 - val_acc: 0.3472\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2276 - acc: 0.3656 - val_loss: 1.1574 - val_acc: 0.3478\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2160 - acc: 0.3704 - val_loss: 1.1515 - val_acc: 0.3583\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2025 - acc: 0.3802 - val_loss: 1.1528 - val_acc: 0.3575\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1902 - acc: 0.3848 - val_loss: 1.1523 - val_acc: 0.3622\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1777 - acc: 0.3860 - val_loss: 1.1514 - val_acc: 0.3617\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1800 - acc: 0.3891 - val_loss: 1.1482 - val_acc: 0.3650\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1570 - acc: 0.4013 - val_loss: 1.1519 - val_acc: 0.3658\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1459 - acc: 0.4090 - val_loss: 1.1525 - val_acc: 0.3646\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1449 - acc: 0.4054 - val_loss: 1.1510 - val_acc: 0.3638\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1451 - acc: 0.4149 - val_loss: 1.1519 - val_acc: 0.3634\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1244 - acc: 0.4181 - val_loss: 1.1515 - val_acc: 0.3612\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1203 - acc: 0.4225 - val_loss: 1.1530 - val_acc: 0.3612\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1154 - acc: 0.4246 - val_loss: 1.1536 - val_acc: 0.3595\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1071 - acc: 0.4285 - val_loss: 1.1539 - val_acc: 0.3592\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1024 - acc: 0.4259 - val_loss: 1.1553 - val_acc: 0.3571\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.0960 - acc: 0.4355 - val_loss: 1.1548 - val_acc: 0.3580\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0938 - acc: 0.4368 - val_loss: 1.1551 - val_acc: 0.3578\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0858 - acc: 0.4393 - val_loss: 1.1540 - val_acc: 0.3569\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0835 - acc: 0.4403 - val_loss: 1.1565 - val_acc: 0.3549\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0766 - acc: 0.4422 - val_loss: 1.1569 - val_acc: 0.3542\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0752 - acc: 0.4454 - val_loss: 1.1562 - val_acc: 0.3524\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0721 - acc: 0.4497 - val_loss: 1.1570 - val_acc: 0.3527\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0647 - acc: 0.4569 - val_loss: 1.1581 - val_acc: 0.3524\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0577 - acc: 0.4565 - val_loss: 1.1575 - val_acc: 0.3539\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0627 - acc: 0.4560 - val_loss: 1.1573 - val_acc: 0.3524\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0568 - acc: 0.4620 - val_loss: 1.1588 - val_acc: 0.3495\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0533 - acc: 0.4578 - val_loss: 1.1599 - val_acc: 0.3478\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0500 - acc: 0.4613 - val_loss: 1.1599 - val_acc: 0.3489\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0427 - acc: 0.4662 - val_loss: 1.1622 - val_acc: 0.3472\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0449 - acc: 0.4633 - val_loss: 1.1614 - val_acc: 0.3471\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0434 - acc: 0.4639 - val_loss: 1.1609 - val_acc: 0.3479\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0327 - acc: 0.4764 - val_loss: 1.1614 - val_acc: 0.3476\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0327 - acc: 0.4750 - val_loss: 1.1623 - val_acc: 0.3466\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0306 - acc: 0.4722 - val_loss: 1.1630 - val_acc: 0.3457\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0306 - acc: 0.4741 - val_loss: 1.1638 - val_acc: 0.3466\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0271 - acc: 0.4731 - val_loss: 1.1630 - val_acc: 0.3481\n",
      "[1.1652893214524604, 1.160945669831632, 1.1578100149897854, 1.15489801853814, 1.15738353774723, 1.1514873764495435, 1.152771283560293, 1.1523222117722847, 1.1514051054738847, 1.1482299907330922, 1.1519372157894623, 1.1525244699836752, 1.1509541321840209, 1.1518929271022371, 1.1515035304451509, 1.1529819923135824, 1.1536492661494324, 1.1538970717292392, 1.155349837336943, 1.1547591530334722, 1.1551375077271007, 1.1539746915936795, 1.1565473833266007, 1.1569215465306586, 1.1561547417081994, 1.1570149796535274, 1.1580741866732813, 1.1574893709099585, 1.1573129096537909, 1.158841890600137, 1.159862114558103, 1.1599291241461314, 1.1622391058898427, 1.1613520627450553, 1.1609183799993115, 1.1614428073249006, 1.1623133375469281, 1.1629738294461118, 1.1637944239686555, 1.1630019833021659]\n",
      "args {'num_dense2': 57}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.3439 - acc: 0.2796 - val_loss: 1.1512 - val_acc: 0.3355\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.2635 - acc: 0.3169 - val_loss: 1.1394 - val_acc: 0.3479\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2237 - acc: 0.3354 - val_loss: 1.1343 - val_acc: 0.3495\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.1990 - acc: 0.3568 - val_loss: 1.1333 - val_acc: 0.3476\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.1839 - acc: 0.3604 - val_loss: 1.1324 - val_acc: 0.3484\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.1743 - acc: 0.3664 - val_loss: 1.1332 - val_acc: 0.3489\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.1564 - acc: 0.3787 - val_loss: 1.1321 - val_acc: 0.3496\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1558 - acc: 0.3845 - val_loss: 1.1321 - val_acc: 0.3520\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1478 - acc: 0.3877 - val_loss: 1.1335 - val_acc: 0.3476\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1357 - acc: 0.3940 - val_loss: 1.1344 - val_acc: 0.3479\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1265 - acc: 0.3972 - val_loss: 1.1337 - val_acc: 0.3515\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1206 - acc: 0.4036 - val_loss: 1.1346 - val_acc: 0.3488\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1173 - acc: 0.4040 - val_loss: 1.1341 - val_acc: 0.3503\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1141 - acc: 0.4048 - val_loss: 1.1337 - val_acc: 0.3486\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1034 - acc: 0.4144 - val_loss: 1.1336 - val_acc: 0.3498\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.0997 - acc: 0.4112 - val_loss: 1.1341 - val_acc: 0.3505\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.0973 - acc: 0.4182 - val_loss: 1.1347 - val_acc: 0.3481\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.0947 - acc: 0.4174 - val_loss: 1.1344 - val_acc: 0.3491\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.0861 - acc: 0.4191 - val_loss: 1.1349 - val_acc: 0.3503\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.0851 - acc: 0.4241 - val_loss: 1.1349 - val_acc: 0.3512\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0824 - acc: 0.4231 - val_loss: 1.1355 - val_acc: 0.3479\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0797 - acc: 0.4268 - val_loss: 1.1341 - val_acc: 0.3462\n",
      "Epoch 23/40\n",
      " - 5s - loss: 1.0764 - acc: 0.4336 - val_loss: 1.1353 - val_acc: 0.3462\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0726 - acc: 0.4323 - val_loss: 1.1356 - val_acc: 0.3455\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0684 - acc: 0.4322 - val_loss: 1.1363 - val_acc: 0.3421\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0649 - acc: 0.4333 - val_loss: 1.1356 - val_acc: 0.3443\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0657 - acc: 0.4350 - val_loss: 1.1373 - val_acc: 0.3404\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0632 - acc: 0.4348 - val_loss: 1.1361 - val_acc: 0.3408\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0579 - acc: 0.4413 - val_loss: 1.1365 - val_acc: 0.3415\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0546 - acc: 0.4459 - val_loss: 1.1376 - val_acc: 0.3418\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0555 - acc: 0.4389 - val_loss: 1.1365 - val_acc: 0.3413\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0518 - acc: 0.4479 - val_loss: 1.1362 - val_acc: 0.3399\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0447 - acc: 0.4518 - val_loss: 1.1371 - val_acc: 0.3384\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0472 - acc: 0.4507 - val_loss: 1.1370 - val_acc: 0.3391\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0396 - acc: 0.4592 - val_loss: 1.1376 - val_acc: 0.3396\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0408 - acc: 0.4511 - val_loss: 1.1380 - val_acc: 0.3409\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0367 - acc: 0.4622 - val_loss: 1.1379 - val_acc: 0.3418\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0376 - acc: 0.4665 - val_loss: 1.1386 - val_acc: 0.3403\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0323 - acc: 0.4606 - val_loss: 1.1389 - val_acc: 0.3396\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0262 - acc: 0.4664 - val_loss: 1.1394 - val_acc: 0.3379\n",
      "[1.1512080580402135, 1.1394386931401184, 1.1343265369087863, 1.133314498114001, 1.1324216055935021, 1.133157635254821, 1.1321218933006723, 1.1321264963383895, 1.1335035520288534, 1.1343662485439705, 1.1337349466796791, 1.1345887664878076, 1.1340715602568125, 1.1337082330797283, 1.133567016196186, 1.1340853079788042, 1.1346755934021453, 1.134439024028726, 1.1348985735661976, 1.1348637764720242, 1.1355377425939575, 1.1341392204612089, 1.1352693147815216, 1.1356168939891889, 1.1363236520855564, 1.135625493948726, 1.1372956316867382, 1.136136694889952, 1.1364677381125718, 1.137556497017759, 1.1364541661187153, 1.1362233321088537, 1.1370574310624957, 1.1370180634127, 1.1376464262970136, 1.1380267786394997, 1.1379187412417877, 1.1385815559356025, 1.138884391057069, 1.139373833866795]\n",
      "args {'num_dense2': 130}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.3184 - acc: 0.2633 - val_loss: 1.1358 - val_acc: 0.3466\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.2625 - acc: 0.3150 - val_loss: 1.1140 - val_acc: 0.3902\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2468 - acc: 0.3349 - val_loss: 1.1078 - val_acc: 0.3976\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2210 - acc: 0.3466 - val_loss: 1.1050 - val_acc: 0.4009\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2081 - acc: 0.3490 - val_loss: 1.1027 - val_acc: 0.4065\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.1975 - acc: 0.3594 - val_loss: 1.1031 - val_acc: 0.4063\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.1814 - acc: 0.3709 - val_loss: 1.1026 - val_acc: 0.4070\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1692 - acc: 0.3757 - val_loss: 1.1039 - val_acc: 0.4028\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1686 - acc: 0.3803 - val_loss: 1.1040 - val_acc: 0.4036\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1539 - acc: 0.3882 - val_loss: 1.1054 - val_acc: 0.4004\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1481 - acc: 0.3925 - val_loss: 1.1068 - val_acc: 0.3990\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1406 - acc: 0.3904 - val_loss: 1.1068 - val_acc: 0.3976\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1309 - acc: 0.3982 - val_loss: 1.1070 - val_acc: 0.3961\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1271 - acc: 0.4099 - val_loss: 1.1093 - val_acc: 0.3903\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1186 - acc: 0.4104 - val_loss: 1.1103 - val_acc: 0.3856\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1181 - acc: 0.4111 - val_loss: 1.1102 - val_acc: 0.3830\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1034 - acc: 0.4183 - val_loss: 1.1119 - val_acc: 0.3804\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.0970 - acc: 0.4201 - val_loss: 1.1147 - val_acc: 0.3772\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.0975 - acc: 0.4232 - val_loss: 1.1157 - val_acc: 0.3779\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.0911 - acc: 0.4257 - val_loss: 1.1182 - val_acc: 0.3769\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0883 - acc: 0.4265 - val_loss: 1.1195 - val_acc: 0.3762\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0796 - acc: 0.4354 - val_loss: 1.1188 - val_acc: 0.3745\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0804 - acc: 0.4322 - val_loss: 1.1200 - val_acc: 0.3719\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0703 - acc: 0.4429 - val_loss: 1.1220 - val_acc: 0.3667\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0721 - acc: 0.4387 - val_loss: 1.1223 - val_acc: 0.3653\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0705 - acc: 0.4378 - val_loss: 1.1236 - val_acc: 0.3651\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0666 - acc: 0.4466 - val_loss: 1.1241 - val_acc: 0.3661\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0653 - acc: 0.4435 - val_loss: 1.1260 - val_acc: 0.3621\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0527 - acc: 0.4463 - val_loss: 1.1258 - val_acc: 0.3612\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0552 - acc: 0.4493 - val_loss: 1.1272 - val_acc: 0.3576\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0489 - acc: 0.4531 - val_loss: 1.1288 - val_acc: 0.3547\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0394 - acc: 0.4585 - val_loss: 1.1286 - val_acc: 0.3556\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0423 - acc: 0.4561 - val_loss: 1.1297 - val_acc: 0.3539\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0401 - acc: 0.4612 - val_loss: 1.1313 - val_acc: 0.3513\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0361 - acc: 0.4602 - val_loss: 1.1319 - val_acc: 0.3537\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0356 - acc: 0.4647 - val_loss: 1.1329 - val_acc: 0.3489\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0340 - acc: 0.4620 - val_loss: 1.1324 - val_acc: 0.3515\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0265 - acc: 0.4698 - val_loss: 1.1336 - val_acc: 0.3489\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0262 - acc: 0.4722 - val_loss: 1.1360 - val_acc: 0.3478\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0240 - acc: 0.4771 - val_loss: 1.1362 - val_acc: 0.3469\n",
      "[1.1358201552476805, 1.113968035505643, 1.1078111852547128, 1.1050483819899182, 1.1027250721928858, 1.1031293706608079, 1.102595881805108, 1.1038881985303166, 1.103976653122447, 1.1053659279275005, 1.1067883240754337, 1.1067670099741755, 1.1070268553674059, 1.1093375744546792, 1.110349473251634, 1.1102428400548991, 1.1119351854766097, 1.1146648112044997, 1.1156523068529383, 1.1181517027379382, 1.1195484146435188, 1.1188487590820977, 1.119975112764322, 1.122005847884134, 1.1223384684372988, 1.1236067565325496, 1.1240510066783396, 1.1260242121096202, 1.1258264555918098, 1.1271992424853166, 1.1287557650002211, 1.1286290383793678, 1.129713356982135, 1.1313177415395628, 1.1318744966704448, 1.1328899126286727, 1.1324311757932242, 1.13362223803185, 1.1359984793520104, 1.1361741551911149]\n",
      "args {'num_dense2': 177}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.4190 - acc: 0.2913 - val_loss: 1.2132 - val_acc: 0.3115\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3244 - acc: 0.3214 - val_loss: 1.1817 - val_acc: 0.3432\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.3114 - acc: 0.3288 - val_loss: 1.1742 - val_acc: 0.3450\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2800 - acc: 0.3465 - val_loss: 1.1678 - val_acc: 0.3486\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2674 - acc: 0.3509 - val_loss: 1.1611 - val_acc: 0.3515\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2594 - acc: 0.3527 - val_loss: 1.1565 - val_acc: 0.3522\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2411 - acc: 0.3629 - val_loss: 1.1524 - val_acc: 0.3537\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.2291 - acc: 0.3622 - val_loss: 1.1492 - val_acc: 0.3583\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.2117 - acc: 0.3692 - val_loss: 1.1487 - val_acc: 0.3532\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.2023 - acc: 0.3725 - val_loss: 1.1473 - val_acc: 0.3503\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1890 - acc: 0.3831 - val_loss: 1.1460 - val_acc: 0.3512\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1791 - acc: 0.3802 - val_loss: 1.1438 - val_acc: 0.3506\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1790 - acc: 0.3918 - val_loss: 1.1428 - val_acc: 0.3493\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1646 - acc: 0.3904 - val_loss: 1.1445 - val_acc: 0.3471\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1523 - acc: 0.3997 - val_loss: 1.1450 - val_acc: 0.3471\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1401 - acc: 0.3992 - val_loss: 1.1448 - val_acc: 0.3495\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1394 - acc: 0.4113 - val_loss: 1.1435 - val_acc: 0.3495\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1335 - acc: 0.4081 - val_loss: 1.1457 - val_acc: 0.3435\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1326 - acc: 0.4057 - val_loss: 1.1442 - val_acc: 0.3469\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1184 - acc: 0.4114 - val_loss: 1.1434 - val_acc: 0.3474\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1087 - acc: 0.4194 - val_loss: 1.1452 - val_acc: 0.3464\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.1151 - acc: 0.4194 - val_loss: 1.1463 - val_acc: 0.3442\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.1034 - acc: 0.4224 - val_loss: 1.1468 - val_acc: 0.3432\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0987 - acc: 0.4249 - val_loss: 1.1477 - val_acc: 0.3420\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0919 - acc: 0.4291 - val_loss: 1.1497 - val_acc: 0.3404\n",
      "Epoch 26/40\n",
      " - 5s - loss: 1.0896 - acc: 0.4317 - val_loss: 1.1494 - val_acc: 0.3425\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0840 - acc: 0.4300 - val_loss: 1.1499 - val_acc: 0.3391\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0777 - acc: 0.4366 - val_loss: 1.1509 - val_acc: 0.3375\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0759 - acc: 0.4398 - val_loss: 1.1515 - val_acc: 0.3357\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0713 - acc: 0.4460 - val_loss: 1.1522 - val_acc: 0.3358\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0709 - acc: 0.4395 - val_loss: 1.1528 - val_acc: 0.3338\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0677 - acc: 0.4447 - val_loss: 1.1519 - val_acc: 0.3350\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0633 - acc: 0.4489 - val_loss: 1.1516 - val_acc: 0.3343\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0566 - acc: 0.4453 - val_loss: 1.1543 - val_acc: 0.3290\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0628 - acc: 0.4492 - val_loss: 1.1557 - val_acc: 0.3290\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0485 - acc: 0.4531 - val_loss: 1.1533 - val_acc: 0.3351\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0518 - acc: 0.4558 - val_loss: 1.1536 - val_acc: 0.3345\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0448 - acc: 0.4596 - val_loss: 1.1548 - val_acc: 0.3348\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0359 - acc: 0.4615 - val_loss: 1.1561 - val_acc: 0.3340\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0359 - acc: 0.4630 - val_loss: 1.1572 - val_acc: 0.3295\n",
      "[1.2131719862083, 1.181703279063877, 1.174209619412955, 1.1678425031396933, 1.1611488349431218, 1.156499531028706, 1.1523788488528384, 1.1492282814810646, 1.1487384979341595, 1.1472836924508742, 1.1459566858224064, 1.1438294022219708, 1.1428131407548037, 1.1444720698962094, 1.1450018424753923, 1.1447786648202007, 1.1434535866537276, 1.145700920180339, 1.1442413411283363, 1.143417634821068, 1.1452117930965788, 1.1462930262251185, 1.1467862561223292, 1.1476844201620657, 1.14970585598283, 1.1493974492075658, 1.14994849268682, 1.1509177275509535, 1.1515182389875198, 1.1522147957570545, 1.1527816102680133, 1.1518946411824031, 1.151594302959598, 1.1542859343806795, 1.1557039370004099, 1.1532958044343165, 1.153563530633495, 1.1547565063923517, 1.1561154474679392, 1.1571909660214623]\n",
      "args {'num_dense2': 281}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.4384 - acc: 0.3766 - val_loss: 1.1710 - val_acc: 0.3406\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3104 - acc: 0.3668 - val_loss: 1.1803 - val_acc: 0.3106\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2751 - acc: 0.3517 - val_loss: 1.1841 - val_acc: 0.2997\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2543 - acc: 0.3589 - val_loss: 1.1871 - val_acc: 0.2951\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2556 - acc: 0.3523 - val_loss: 1.1818 - val_acc: 0.2980\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2306 - acc: 0.3645 - val_loss: 1.1770 - val_acc: 0.3042\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2183 - acc: 0.3692 - val_loss: 1.1781 - val_acc: 0.3007\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1958 - acc: 0.3738 - val_loss: 1.1757 - val_acc: 0.3050\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1935 - acc: 0.3821 - val_loss: 1.1715 - val_acc: 0.3099\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1845 - acc: 0.3879 - val_loss: 1.1724 - val_acc: 0.3077\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1638 - acc: 0.3899 - val_loss: 1.1707 - val_acc: 0.3140\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1627 - acc: 0.3990 - val_loss: 1.1726 - val_acc: 0.3098\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1507 - acc: 0.3999 - val_loss: 1.1700 - val_acc: 0.3135\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1403 - acc: 0.3999 - val_loss: 1.1689 - val_acc: 0.3152\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1357 - acc: 0.4056 - val_loss: 1.1670 - val_acc: 0.3178\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1278 - acc: 0.4145 - val_loss: 1.1646 - val_acc: 0.3214\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1215 - acc: 0.4175 - val_loss: 1.1670 - val_acc: 0.3195\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1140 - acc: 0.4166 - val_loss: 1.1668 - val_acc: 0.3195\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1026 - acc: 0.4258 - val_loss: 1.1652 - val_acc: 0.3214\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1027 - acc: 0.4281 - val_loss: 1.1633 - val_acc: 0.3306\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1000 - acc: 0.4276 - val_loss: 1.1641 - val_acc: 0.3299\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0880 - acc: 0.4357 - val_loss: 1.1627 - val_acc: 0.3309\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0849 - acc: 0.4326 - val_loss: 1.1635 - val_acc: 0.3297\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0873 - acc: 0.4392 - val_loss: 1.1652 - val_acc: 0.3251\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0852 - acc: 0.4363 - val_loss: 1.1632 - val_acc: 0.3285\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0753 - acc: 0.4405 - val_loss: 1.1636 - val_acc: 0.3266\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0762 - acc: 0.4396 - val_loss: 1.1614 - val_acc: 0.3312\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0674 - acc: 0.4464 - val_loss: 1.1617 - val_acc: 0.3314\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0663 - acc: 0.4471 - val_loss: 1.1632 - val_acc: 0.3292\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0591 - acc: 0.4564 - val_loss: 1.1626 - val_acc: 0.3312\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0597 - acc: 0.4517 - val_loss: 1.1627 - val_acc: 0.3300\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0584 - acc: 0.4586 - val_loss: 1.1631 - val_acc: 0.3321\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0510 - acc: 0.4538 - val_loss: 1.1631 - val_acc: 0.3340\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0472 - acc: 0.4636 - val_loss: 1.1630 - val_acc: 0.3333\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0446 - acc: 0.4689 - val_loss: 1.1625 - val_acc: 0.3357\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0379 - acc: 0.4632 - val_loss: 1.1640 - val_acc: 0.3357\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0396 - acc: 0.4701 - val_loss: 1.1638 - val_acc: 0.3353\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0294 - acc: 0.4687 - val_loss: 1.1633 - val_acc: 0.3382\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0285 - acc: 0.4733 - val_loss: 1.1641 - val_acc: 0.3358\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0256 - acc: 0.4758 - val_loss: 1.1646 - val_acc: 0.3365\n",
      "[1.1710057736092758, 1.1803249598199081, 1.1840512362747817, 1.1871222959227392, 1.1817955187945666, 1.1770452839152366, 1.178129683398421, 1.1756840158223456, 1.1715392069205925, 1.1723653594544539, 1.1706925779987096, 1.1725967329919176, 1.170000906211479, 1.1689058876817169, 1.1669729064530832, 1.164598666354811, 1.1670015098613356, 1.1667820458840934, 1.1652017373804826, 1.1633248001093435, 1.1640676484120964, 1.1627007839790158, 1.163457135413583, 1.165150296785526, 1.1632110117566683, 1.1636475399339556, 1.1614304504862274, 1.1616924640593151, 1.1631648930281968, 1.162582530637528, 1.162672211753575, 1.1631380816571395, 1.1630954245455583, 1.1629524266687337, 1.1624712625706228, 1.1639717508726615, 1.1638169327613768, 1.1632663339620066, 1.164147767448945, 1.164590459428626]\n",
      "args {'num_dense2': 206}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.7570 - acc: 0.2604 - val_loss: 1.3738 - val_acc: 0.2585\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.4392 - acc: 0.2669 - val_loss: 1.2117 - val_acc: 0.2994\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.3127 - acc: 0.3114 - val_loss: 1.1515 - val_acc: 0.3770\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2741 - acc: 0.3427 - val_loss: 1.1337 - val_acc: 0.4009\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2619 - acc: 0.3645 - val_loss: 1.1283 - val_acc: 0.4045\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2485 - acc: 0.3661 - val_loss: 1.1254 - val_acc: 0.4038\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2348 - acc: 0.3703 - val_loss: 1.1230 - val_acc: 0.4057\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.2161 - acc: 0.3805 - val_loss: 1.1215 - val_acc: 0.4050\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.2100 - acc: 0.3843 - val_loss: 1.1218 - val_acc: 0.4004\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1872 - acc: 0.3889 - val_loss: 1.1192 - val_acc: 0.4057\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1903 - acc: 0.3878 - val_loss: 1.1186 - val_acc: 0.4033\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1717 - acc: 0.3969 - val_loss: 1.1184 - val_acc: 0.3994\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1594 - acc: 0.4104 - val_loss: 1.1192 - val_acc: 0.3915\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1633 - acc: 0.4041 - val_loss: 1.1203 - val_acc: 0.3869\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1475 - acc: 0.4070 - val_loss: 1.1223 - val_acc: 0.3840\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1354 - acc: 0.4121 - val_loss: 1.1221 - val_acc: 0.3830\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1340 - acc: 0.4108 - val_loss: 1.1208 - val_acc: 0.3842\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1308 - acc: 0.4197 - val_loss: 1.1235 - val_acc: 0.3818\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1215 - acc: 0.4220 - val_loss: 1.1252 - val_acc: 0.3786\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1201 - acc: 0.4228 - val_loss: 1.1264 - val_acc: 0.3776\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1098 - acc: 0.4241 - val_loss: 1.1273 - val_acc: 0.3765\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.1030 - acc: 0.4294 - val_loss: 1.1266 - val_acc: 0.3762\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.1038 - acc: 0.4336 - val_loss: 1.1288 - val_acc: 0.3748\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.1002 - acc: 0.4279 - val_loss: 1.1295 - val_acc: 0.3724\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0922 - acc: 0.4390 - val_loss: 1.1314 - val_acc: 0.3713\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0845 - acc: 0.4382 - val_loss: 1.1331 - val_acc: 0.3694\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0815 - acc: 0.4484 - val_loss: 1.1340 - val_acc: 0.3668\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0783 - acc: 0.4405 - val_loss: 1.1356 - val_acc: 0.3661\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0703 - acc: 0.4418 - val_loss: 1.1364 - val_acc: 0.3694\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0743 - acc: 0.4475 - val_loss: 1.1378 - val_acc: 0.3684\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0680 - acc: 0.4477 - val_loss: 1.1394 - val_acc: 0.3648\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0603 - acc: 0.4518 - val_loss: 1.1401 - val_acc: 0.3656\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0577 - acc: 0.4578 - val_loss: 1.1405 - val_acc: 0.3655\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0562 - acc: 0.4533 - val_loss: 1.1422 - val_acc: 0.3629\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0498 - acc: 0.4598 - val_loss: 1.1432 - val_acc: 0.3595\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0438 - acc: 0.4615 - val_loss: 1.1446 - val_acc: 0.3566\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0406 - acc: 0.4718 - val_loss: 1.1434 - val_acc: 0.3578\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0465 - acc: 0.4644 - val_loss: 1.1458 - val_acc: 0.3546\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0374 - acc: 0.4671 - val_loss: 1.1473 - val_acc: 0.3546\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0381 - acc: 0.4667 - val_loss: 1.1478 - val_acc: 0.3517\n",
      "[1.3737733484288976, 1.2117219552681946, 1.1515130473742368, 1.1336847017506488, 1.1282782697547684, 1.1253642876726404, 1.12302613323326, 1.121483565351294, 1.1217904311759594, 1.1191733805944875, 1.1185603222989906, 1.1184153612043293, 1.1191685716202864, 1.1203286287245373, 1.122277063634805, 1.122050083300723, 1.120822589468891, 1.1234707026780464, 1.1252172610415425, 1.1264422703179091, 1.1272808277639446, 1.1266095056845642, 1.1287801603530343, 1.1295300078976707, 1.13142897614991, 1.1330916904298747, 1.1340231216570986, 1.1355544923111918, 1.1364050093398756, 1.1377717620345487, 1.1393758464574164, 1.1400563327103286, 1.1404708659616414, 1.1421983284261636, 1.1432065564215346, 1.1445577472691966, 1.1433792757403298, 1.1458085460299043, 1.1473491253579995, 1.1478215797070912]\n",
      "args {'num_dense2': 79}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.3214 - acc: 0.2963 - val_loss: 1.1791 - val_acc: 0.2868\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.2805 - acc: 0.3353 - val_loss: 1.1639 - val_acc: 0.3001\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2487 - acc: 0.3500 - val_loss: 1.1540 - val_acc: 0.3094\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2265 - acc: 0.3680 - val_loss: 1.1468 - val_acc: 0.3162\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2112 - acc: 0.3736 - val_loss: 1.1431 - val_acc: 0.3181\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.1946 - acc: 0.3750 - val_loss: 1.1407 - val_acc: 0.3232\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.1848 - acc: 0.3792 - val_loss: 1.1365 - val_acc: 0.3270\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1714 - acc: 0.3871 - val_loss: 1.1351 - val_acc: 0.3336\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1640 - acc: 0.3923 - val_loss: 1.1322 - val_acc: 0.3369\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1507 - acc: 0.3955 - val_loss: 1.1299 - val_acc: 0.3443\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1413 - acc: 0.4060 - val_loss: 1.1293 - val_acc: 0.3460\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1360 - acc: 0.4077 - val_loss: 1.1286 - val_acc: 0.3498\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1275 - acc: 0.4136 - val_loss: 1.1286 - val_acc: 0.3515\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1155 - acc: 0.4245 - val_loss: 1.1289 - val_acc: 0.3508\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1142 - acc: 0.4174 - val_loss: 1.1286 - val_acc: 0.3481\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1068 - acc: 0.4200 - val_loss: 1.1284 - val_acc: 0.3515\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1054 - acc: 0.4218 - val_loss: 1.1272 - val_acc: 0.3541\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.0970 - acc: 0.4277 - val_loss: 1.1281 - val_acc: 0.3500\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.0888 - acc: 0.4283 - val_loss: 1.1285 - val_acc: 0.3512\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.0911 - acc: 0.4289 - val_loss: 1.1272 - val_acc: 0.3535\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0931 - acc: 0.4313 - val_loss: 1.1287 - val_acc: 0.3529\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0846 - acc: 0.4363 - val_loss: 1.1288 - val_acc: 0.3529\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0781 - acc: 0.4364 - val_loss: 1.1293 - val_acc: 0.3505\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0725 - acc: 0.4412 - val_loss: 1.1295 - val_acc: 0.3527\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0728 - acc: 0.4480 - val_loss: 1.1304 - val_acc: 0.3532\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0656 - acc: 0.4467 - val_loss: 1.1310 - val_acc: 0.3522\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0591 - acc: 0.4547 - val_loss: 1.1325 - val_acc: 0.3508\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0640 - acc: 0.4471 - val_loss: 1.1335 - val_acc: 0.3524\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0605 - acc: 0.4483 - val_loss: 1.1334 - val_acc: 0.3489\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0539 - acc: 0.4569 - val_loss: 1.1329 - val_acc: 0.3518\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0533 - acc: 0.4568 - val_loss: 1.1347 - val_acc: 0.3503\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0512 - acc: 0.4598 - val_loss: 1.1351 - val_acc: 0.3505\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0522 - acc: 0.4564 - val_loss: 1.1373 - val_acc: 0.3466\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0465 - acc: 0.4546 - val_loss: 1.1369 - val_acc: 0.3471\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0412 - acc: 0.4632 - val_loss: 1.1363 - val_acc: 0.3500\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0396 - acc: 0.4660 - val_loss: 1.1357 - val_acc: 0.3513\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0328 - acc: 0.4696 - val_loss: 1.1377 - val_acc: 0.3478\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0388 - acc: 0.4628 - val_loss: 1.1381 - val_acc: 0.3464\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0357 - acc: 0.4671 - val_loss: 1.1382 - val_acc: 0.3467\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0270 - acc: 0.4787 - val_loss: 1.1394 - val_acc: 0.3443\n",
      "[1.179117886831715, 1.163862472009269, 1.1539742703009042, 1.146778756006537, 1.1431192071951053, 1.1406864674929378, 1.1364792244310924, 1.1351037808270155, 1.1322420491184786, 1.1299093276343488, 1.1292958480460766, 1.1285578792036717, 1.1286240179467266, 1.1288564247396402, 1.128572468536751, 1.1283504914198, 1.1271957740471863, 1.1281250848432327, 1.1284657076853821, 1.127196583176179, 1.1286950163360512, 1.1288040207257388, 1.1292693705909584, 1.1294567195206313, 1.130350407202822, 1.1310326604817154, 1.1324658663461253, 1.133465222182001, 1.1333777440016537, 1.1329232717404898, 1.1347239751581926, 1.1351343328361616, 1.1373395669687671, 1.1369127018899943, 1.1362738245514499, 1.1356531810370714, 1.1377140644135852, 1.1380584464086174, 1.1382228808441994, 1.1394096999467231]\n",
      "args {'num_dense2': 63}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.2923 - acc: 0.2921 - val_loss: 1.1555 - val_acc: 0.3212\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.2390 - acc: 0.3414 - val_loss: 1.1257 - val_acc: 0.3636\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2192 - acc: 0.3601 - val_loss: 1.1150 - val_acc: 0.3781\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2081 - acc: 0.3638 - val_loss: 1.1107 - val_acc: 0.3796\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.1878 - acc: 0.3777 - val_loss: 1.1064 - val_acc: 0.3871\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.1784 - acc: 0.3829 - val_loss: 1.1051 - val_acc: 0.3886\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.1659 - acc: 0.3826 - val_loss: 1.1022 - val_acc: 0.3937\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1564 - acc: 0.3883 - val_loss: 1.1002 - val_acc: 0.3961\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1419 - acc: 0.4014 - val_loss: 1.1000 - val_acc: 0.3941\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1438 - acc: 0.4004 - val_loss: 1.0995 - val_acc: 0.3963\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1257 - acc: 0.4004 - val_loss: 1.0993 - val_acc: 0.3961\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1217 - acc: 0.4027 - val_loss: 1.0979 - val_acc: 0.3980\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1114 - acc: 0.4119 - val_loss: 1.0980 - val_acc: 0.3961\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1137 - acc: 0.4130 - val_loss: 1.0986 - val_acc: 0.3944\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1028 - acc: 0.4196 - val_loss: 1.0987 - val_acc: 0.3915\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.0966 - acc: 0.4237 - val_loss: 1.0995 - val_acc: 0.3888\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.0915 - acc: 0.4246 - val_loss: 1.1008 - val_acc: 0.3893\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.0891 - acc: 0.4330 - val_loss: 1.1008 - val_acc: 0.3890\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.0850 - acc: 0.4338 - val_loss: 1.1013 - val_acc: 0.3837\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.0836 - acc: 0.4329 - val_loss: 1.1013 - val_acc: 0.3825\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0777 - acc: 0.4384 - val_loss: 1.1027 - val_acc: 0.3813\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0732 - acc: 0.4359 - val_loss: 1.1040 - val_acc: 0.3806\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0731 - acc: 0.4343 - val_loss: 1.1037 - val_acc: 0.3808\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0633 - acc: 0.4493 - val_loss: 1.1043 - val_acc: 0.3796\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0674 - acc: 0.4451 - val_loss: 1.1054 - val_acc: 0.3787\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0564 - acc: 0.4517 - val_loss: 1.1071 - val_acc: 0.3762\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0551 - acc: 0.4509 - val_loss: 1.1069 - val_acc: 0.3776\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0517 - acc: 0.4551 - val_loss: 1.1084 - val_acc: 0.3772\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0464 - acc: 0.4568 - val_loss: 1.1084 - val_acc: 0.3770\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0485 - acc: 0.4539 - val_loss: 1.1092 - val_acc: 0.3736\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0429 - acc: 0.4570 - val_loss: 1.1107 - val_acc: 0.3721\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0449 - acc: 0.4588 - val_loss: 1.1115 - val_acc: 0.3711\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0414 - acc: 0.4689 - val_loss: 1.1118 - val_acc: 0.3714\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0409 - acc: 0.4657 - val_loss: 1.1121 - val_acc: 0.3706\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0397 - acc: 0.4635 - val_loss: 1.1128 - val_acc: 0.3689\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0331 - acc: 0.4701 - val_loss: 1.1133 - val_acc: 0.3697\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0303 - acc: 0.4717 - val_loss: 1.1136 - val_acc: 0.3719\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0275 - acc: 0.4721 - val_loss: 1.1141 - val_acc: 0.3702\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0254 - acc: 0.4746 - val_loss: 1.1145 - val_acc: 0.3673\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0206 - acc: 0.4790 - val_loss: 1.1161 - val_acc: 0.3646\n",
      "[1.1554866464001605, 1.1256537976641745, 1.1149800444169005, 1.1107341528588486, 1.1063912197419667, 1.1050631814821539, 1.1021675608788264, 1.1002410647005085, 1.099969477679489, 1.0995339246147011, 1.099302982439462, 1.0979480944797193, 1.0980382469109684, 1.0986292378454183, 1.0987345064693317, 1.099515179522356, 1.1007695945147273, 1.1007558623841414, 1.1013427066543122, 1.1013171286284111, 1.1027228819252035, 1.1040310382193375, 1.1036816598284147, 1.1042753834815375, 1.1054005840493808, 1.1071002668515864, 1.1069150464736148, 1.1083658478890193, 1.1084119978003022, 1.109159866860517, 1.1107320535410328, 1.1114777355817749, 1.1118466071276965, 1.1121469398285453, 1.1128026870680765, 1.1133094347140444, 1.113609392896335, 1.114087085957748, 1.1145351771113008, 1.116140023888944]\n",
      "args {'num_dense2': 331}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.4760 - acc: 0.4445 - val_loss: 1.1077 - val_acc: 0.4091\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3078 - acc: 0.3937 - val_loss: 1.1465 - val_acc: 0.3421\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2692 - acc: 0.3661 - val_loss: 1.1650 - val_acc: 0.3147\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2489 - acc: 0.3476 - val_loss: 1.1696 - val_acc: 0.3106\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2308 - acc: 0.3607 - val_loss: 1.1663 - val_acc: 0.3132\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2171 - acc: 0.3673 - val_loss: 1.1693 - val_acc: 0.3082\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2131 - acc: 0.3645 - val_loss: 1.1676 - val_acc: 0.3101\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.2038 - acc: 0.3724 - val_loss: 1.1685 - val_acc: 0.3128\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1929 - acc: 0.3737 - val_loss: 1.1669 - val_acc: 0.3123\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1797 - acc: 0.3758 - val_loss: 1.1669 - val_acc: 0.3132\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1652 - acc: 0.3780 - val_loss: 1.1665 - val_acc: 0.3128\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1624 - acc: 0.3911 - val_loss: 1.1652 - val_acc: 0.3111\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1481 - acc: 0.3968 - val_loss: 1.1634 - val_acc: 0.3142\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1406 - acc: 0.4010 - val_loss: 1.1661 - val_acc: 0.3110\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1388 - acc: 0.3987 - val_loss: 1.1652 - val_acc: 0.3132\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1360 - acc: 0.4028 - val_loss: 1.1650 - val_acc: 0.3122\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1193 - acc: 0.4101 - val_loss: 1.1643 - val_acc: 0.3157\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1187 - acc: 0.4143 - val_loss: 1.1639 - val_acc: 0.3171\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1161 - acc: 0.4134 - val_loss: 1.1648 - val_acc: 0.3151\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1090 - acc: 0.4185 - val_loss: 1.1654 - val_acc: 0.3173\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1059 - acc: 0.4182 - val_loss: 1.1668 - val_acc: 0.3152\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0991 - acc: 0.4172 - val_loss: 1.1662 - val_acc: 0.3185\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0875 - acc: 0.4240 - val_loss: 1.1651 - val_acc: 0.3198\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0842 - acc: 0.4315 - val_loss: 1.1653 - val_acc: 0.3176\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0815 - acc: 0.4339 - val_loss: 1.1679 - val_acc: 0.3144\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0715 - acc: 0.4360 - val_loss: 1.1677 - val_acc: 0.3154\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0753 - acc: 0.4367 - val_loss: 1.1682 - val_acc: 0.3152\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0678 - acc: 0.4424 - val_loss: 1.1685 - val_acc: 0.3140\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0652 - acc: 0.4449 - val_loss: 1.1673 - val_acc: 0.3145\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0610 - acc: 0.4404 - val_loss: 1.1680 - val_acc: 0.3134\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0544 - acc: 0.4472 - val_loss: 1.1690 - val_acc: 0.3154\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0562 - acc: 0.4513 - val_loss: 1.1686 - val_acc: 0.3166\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0473 - acc: 0.4509 - val_loss: 1.1685 - val_acc: 0.3178\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0480 - acc: 0.4542 - val_loss: 1.1692 - val_acc: 0.3161\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0479 - acc: 0.4543 - val_loss: 1.1713 - val_acc: 0.3135\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0404 - acc: 0.4565 - val_loss: 1.1704 - val_acc: 0.3145\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0340 - acc: 0.4645 - val_loss: 1.1693 - val_acc: 0.3171\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0415 - acc: 0.4572 - val_loss: 1.1717 - val_acc: 0.3132\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0305 - acc: 0.4625 - val_loss: 1.1715 - val_acc: 0.3147\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0278 - acc: 0.4739 - val_loss: 1.1708 - val_acc: 0.3171\n",
      "[1.1076975394984356, 1.1465111035417146, 1.16504502036591, 1.169614458928641, 1.1663054533160675, 1.169349849711322, 1.167551513588721, 1.168476400323395, 1.1669115260121607, 1.1668603394271893, 1.1665413782447172, 1.1651903407125448, 1.1634411811828613, 1.166066829125303, 1.165178929427664, 1.1650168145385036, 1.1642904642167469, 1.1639333684048145, 1.1648394762657643, 1.1654371211574253, 1.166840712445958, 1.16622003157717, 1.1651485794571506, 1.165329222783081, 1.1678625013912731, 1.1676728946311596, 1.1681601075450472, 1.1685175859960613, 1.1672873243648934, 1.1680472588993873, 1.1690293950021104, 1.168642999040983, 1.1684587284394765, 1.1691950527133994, 1.1713379972312365, 1.1704379654710235, 1.1692781126791514, 1.1717042016723176, 1.1714681633811557, 1.1707677617060066]\n",
      "args {'num_dense2': 230}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.4919 - acc: 0.3474 - val_loss: 1.1632 - val_acc: 0.3495\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3130 - acc: 0.3642 - val_loss: 1.1547 - val_acc: 0.3171\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2571 - acc: 0.3597 - val_loss: 1.1568 - val_acc: 0.2987\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2498 - acc: 0.3534 - val_loss: 1.1593 - val_acc: 0.2873\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2271 - acc: 0.3601 - val_loss: 1.1573 - val_acc: 0.2866\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2151 - acc: 0.3633 - val_loss: 1.1544 - val_acc: 0.2912\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2025 - acc: 0.3715 - val_loss: 1.1504 - val_acc: 0.2934\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1872 - acc: 0.3822 - val_loss: 1.1488 - val_acc: 0.2968\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1716 - acc: 0.3870 - val_loss: 1.1453 - val_acc: 0.3033\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1726 - acc: 0.3887 - val_loss: 1.1449 - val_acc: 0.3043\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1545 - acc: 0.4044 - val_loss: 1.1451 - val_acc: 0.3081\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1496 - acc: 0.4021 - val_loss: 1.1434 - val_acc: 0.3140\n",
      "Epoch 13/40\n",
      " - 5s - loss: 1.1464 - acc: 0.4040 - val_loss: 1.1440 - val_acc: 0.3168\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1280 - acc: 0.4136 - val_loss: 1.1424 - val_acc: 0.3227\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1316 - acc: 0.4051 - val_loss: 1.1416 - val_acc: 0.3285\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1227 - acc: 0.4163 - val_loss: 1.1425 - val_acc: 0.3271\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1193 - acc: 0.4253 - val_loss: 1.1437 - val_acc: 0.3300\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1108 - acc: 0.4232 - val_loss: 1.1411 - val_acc: 0.3362\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.0984 - acc: 0.4302 - val_loss: 1.1406 - val_acc: 0.3375\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1050 - acc: 0.4222 - val_loss: 1.1416 - val_acc: 0.3389\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0977 - acc: 0.4292 - val_loss: 1.1410 - val_acc: 0.3396\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0950 - acc: 0.4354 - val_loss: 1.1425 - val_acc: 0.3415\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0864 - acc: 0.4385 - val_loss: 1.1424 - val_acc: 0.3420\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0808 - acc: 0.4401 - val_loss: 1.1422 - val_acc: 0.3433\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0826 - acc: 0.4438 - val_loss: 1.1433 - val_acc: 0.3408\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0698 - acc: 0.4473 - val_loss: 1.1423 - val_acc: 0.3443\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0675 - acc: 0.4493 - val_loss: 1.1430 - val_acc: 0.3445\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0679 - acc: 0.4496 - val_loss: 1.1434 - val_acc: 0.3455\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0637 - acc: 0.4515 - val_loss: 1.1412 - val_acc: 0.3500\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0647 - acc: 0.4510 - val_loss: 1.1438 - val_acc: 0.3455\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0595 - acc: 0.4594 - val_loss: 1.1432 - val_acc: 0.3495\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0544 - acc: 0.4560 - val_loss: 1.1447 - val_acc: 0.3501\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0497 - acc: 0.4594 - val_loss: 1.1437 - val_acc: 0.3506\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0484 - acc: 0.4580 - val_loss: 1.1442 - val_acc: 0.3495\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0470 - acc: 0.4656 - val_loss: 1.1455 - val_acc: 0.3486\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0438 - acc: 0.4651 - val_loss: 1.1457 - val_acc: 0.3481\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0343 - acc: 0.4720 - val_loss: 1.1452 - val_acc: 0.3481\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0335 - acc: 0.4747 - val_loss: 1.1453 - val_acc: 0.3472\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0315 - acc: 0.4732 - val_loss: 1.1436 - val_acc: 0.3506\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0292 - acc: 0.4757 - val_loss: 1.1453 - val_acc: 0.3476\n",
      "[1.1631641397684083, 1.154683335925318, 1.15677205743192, 1.1593121030350146, 1.1572749367851651, 1.1544496032132765, 1.1504433986601452, 1.14883080620207, 1.1452605990687899, 1.1449244642777403, 1.145071630257027, 1.1433996998321783, 1.143973106584367, 1.142427812155326, 1.1416325549663575, 1.142465564795346, 1.1436807323216742, 1.141134350111439, 1.1405663854094876, 1.1416171630007044, 1.1409579141913058, 1.1425096891231692, 1.142402956856044, 1.1421686596051874, 1.1433302017908331, 1.1422726276460071, 1.143012189735184, 1.1433927083859976, 1.1411796441195121, 1.1437938508935455, 1.143179690155736, 1.144657602751937, 1.1437221794752075, 1.1441806405376673, 1.1454556816605197, 1.145656643186668, 1.1451779412313767, 1.145287740457935, 1.143610701249146, 1.145261800256672]\n",
      "args {'num_dense2': 98}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.4062 - acc: 0.2966 - val_loss: 1.1801 - val_acc: 0.3035\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3113 - acc: 0.3471 - val_loss: 1.1593 - val_acc: 0.3333\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2850 - acc: 0.3578 - val_loss: 1.1508 - val_acc: 0.3452\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2689 - acc: 0.3606 - val_loss: 1.1465 - val_acc: 0.3445\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2415 - acc: 0.3709 - val_loss: 1.1399 - val_acc: 0.3491\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2300 - acc: 0.3725 - val_loss: 1.1358 - val_acc: 0.3532\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2147 - acc: 0.3796 - val_loss: 1.1346 - val_acc: 0.3530\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1933 - acc: 0.3868 - val_loss: 1.1311 - val_acc: 0.3587\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1835 - acc: 0.3948 - val_loss: 1.1292 - val_acc: 0.3622\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1730 - acc: 0.4020 - val_loss: 1.1290 - val_acc: 0.3583\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1624 - acc: 0.3992 - val_loss: 1.1285 - val_acc: 0.3585\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1501 - acc: 0.4057 - val_loss: 1.1270 - val_acc: 0.3592\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1414 - acc: 0.4098 - val_loss: 1.1246 - val_acc: 0.3624\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1401 - acc: 0.4102 - val_loss: 1.1247 - val_acc: 0.3667\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1319 - acc: 0.4193 - val_loss: 1.1242 - val_acc: 0.3672\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1231 - acc: 0.4224 - val_loss: 1.1249 - val_acc: 0.3677\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1189 - acc: 0.4231 - val_loss: 1.1243 - val_acc: 0.3678\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1146 - acc: 0.4275 - val_loss: 1.1263 - val_acc: 0.3638\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1052 - acc: 0.4304 - val_loss: 1.1269 - val_acc: 0.3626\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1058 - acc: 0.4247 - val_loss: 1.1268 - val_acc: 0.3639\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0958 - acc: 0.4325 - val_loss: 1.1252 - val_acc: 0.3660\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0919 - acc: 0.4410 - val_loss: 1.1259 - val_acc: 0.3653\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0916 - acc: 0.4380 - val_loss: 1.1275 - val_acc: 0.3626\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0839 - acc: 0.4402 - val_loss: 1.1278 - val_acc: 0.3632\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0784 - acc: 0.4449 - val_loss: 1.1271 - val_acc: 0.3629\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0722 - acc: 0.4493 - val_loss: 1.1279 - val_acc: 0.3610\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0760 - acc: 0.4476 - val_loss: 1.1291 - val_acc: 0.3575\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0698 - acc: 0.4493 - val_loss: 1.1292 - val_acc: 0.3573\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0634 - acc: 0.4510 - val_loss: 1.1279 - val_acc: 0.3573\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0603 - acc: 0.4553 - val_loss: 1.1298 - val_acc: 0.3549\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0583 - acc: 0.4587 - val_loss: 1.1312 - val_acc: 0.3539\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0525 - acc: 0.4526 - val_loss: 1.1323 - val_acc: 0.3556\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0520 - acc: 0.4601 - val_loss: 1.1325 - val_acc: 0.3546\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0463 - acc: 0.4633 - val_loss: 1.1321 - val_acc: 0.3552\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0479 - acc: 0.4611 - val_loss: 1.1325 - val_acc: 0.3551\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0403 - acc: 0.4643 - val_loss: 1.1337 - val_acc: 0.3530\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0388 - acc: 0.4649 - val_loss: 1.1359 - val_acc: 0.3495\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0370 - acc: 0.4684 - val_loss: 1.1340 - val_acc: 0.3522\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0311 - acc: 0.4690 - val_loss: 1.1364 - val_acc: 0.3496\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0303 - acc: 0.4754 - val_loss: 1.1377 - val_acc: 0.3471\n",
      "[1.180075640574463, 1.1593270675362943, 1.1508115354610724, 1.1465231089241172, 1.139918715817402, 1.1358025626200745, 1.1345521773564393, 1.1311023852480855, 1.1291713337807305, 1.1290283833277648, 1.1284968511934825, 1.1269708521034802, 1.124604549654823, 1.124663911333526, 1.1242034038990656, 1.1248771280293894, 1.1242569291948947, 1.1263229781990156, 1.126863641375092, 1.1267915450909483, 1.125212940273233, 1.1258929220792058, 1.1274894589624223, 1.1277649610503817, 1.1270644245745376, 1.1279396975072917, 1.1290674359012365, 1.1291968107873154, 1.1279199779520892, 1.129809090486989, 1.1311722948375775, 1.1323129685763118, 1.1324647138813861, 1.1321410340249376, 1.132499721459537, 1.1337079238501817, 1.1359042293044461, 1.134025728670063, 1.136398003601573, 1.1377046822851944]\n",
      "args {'num_dense2': 255}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.3479 - acc: 0.3218 - val_loss: 1.1654 - val_acc: 0.3149\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3168 - acc: 0.3102 - val_loss: 1.1600 - val_acc: 0.3231\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2868 - acc: 0.3235 - val_loss: 1.1560 - val_acc: 0.3217\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2679 - acc: 0.3225 - val_loss: 1.1509 - val_acc: 0.3266\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2461 - acc: 0.3372 - val_loss: 1.1465 - val_acc: 0.3328\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2344 - acc: 0.3380 - val_loss: 1.1449 - val_acc: 0.3351\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2158 - acc: 0.3551 - val_loss: 1.1435 - val_acc: 0.3380\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1953 - acc: 0.3551 - val_loss: 1.1435 - val_acc: 0.3365\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1945 - acc: 0.3663 - val_loss: 1.1425 - val_acc: 0.3394\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1712 - acc: 0.3752 - val_loss: 1.1430 - val_acc: 0.3435\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1686 - acc: 0.3706 - val_loss: 1.1428 - val_acc: 0.3449\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1541 - acc: 0.3836 - val_loss: 1.1419 - val_acc: 0.3459\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1451 - acc: 0.3895 - val_loss: 1.1433 - val_acc: 0.3469\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1411 - acc: 0.3873 - val_loss: 1.1423 - val_acc: 0.3495\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1359 - acc: 0.3900 - val_loss: 1.1439 - val_acc: 0.3501\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1298 - acc: 0.3925 - val_loss: 1.1434 - val_acc: 0.3522\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1235 - acc: 0.4046 - val_loss: 1.1445 - val_acc: 0.3500\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1149 - acc: 0.4003 - val_loss: 1.1463 - val_acc: 0.3464\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1040 - acc: 0.4056 - val_loss: 1.1458 - val_acc: 0.3488\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1034 - acc: 0.4150 - val_loss: 1.1466 - val_acc: 0.3498\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0998 - acc: 0.4142 - val_loss: 1.1474 - val_acc: 0.3503\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0929 - acc: 0.4157 - val_loss: 1.1482 - val_acc: 0.3495\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0936 - acc: 0.4239 - val_loss: 1.1484 - val_acc: 0.3503\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0894 - acc: 0.4234 - val_loss: 1.1488 - val_acc: 0.3493\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0773 - acc: 0.4300 - val_loss: 1.1494 - val_acc: 0.3488\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0741 - acc: 0.4338 - val_loss: 1.1489 - val_acc: 0.3512\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0759 - acc: 0.4333 - val_loss: 1.1497 - val_acc: 0.3488\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0648 - acc: 0.4404 - val_loss: 1.1494 - val_acc: 0.3500\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0657 - acc: 0.4311 - val_loss: 1.1498 - val_acc: 0.3501\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0580 - acc: 0.4451 - val_loss: 1.1506 - val_acc: 0.3503\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0620 - acc: 0.4427 - val_loss: 1.1511 - val_acc: 0.3493\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0538 - acc: 0.4463 - val_loss: 1.1534 - val_acc: 0.3464\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0515 - acc: 0.4465 - val_loss: 1.1546 - val_acc: 0.3466\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0460 - acc: 0.4503 - val_loss: 1.1543 - val_acc: 0.3464\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0501 - acc: 0.4489 - val_loss: 1.1534 - val_acc: 0.3476\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0506 - acc: 0.4569 - val_loss: 1.1558 - val_acc: 0.3438\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0429 - acc: 0.4589 - val_loss: 1.1563 - val_acc: 0.3442\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0361 - acc: 0.4595 - val_loss: 1.1546 - val_acc: 0.3457\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0348 - acc: 0.4642 - val_loss: 1.1563 - val_acc: 0.3443\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0337 - acc: 0.4611 - val_loss: 1.1561 - val_acc: 0.3442\n",
      "[1.1653987133535442, 1.1599705326459713, 1.1559744284003568, 1.1508619843776609, 1.1464753323095047, 1.1449409456928679, 1.1434618725114039, 1.1434959631200057, 1.1424761078338208, 1.1430255980192803, 1.1428119814688242, 1.141903049614514, 1.1433088896385006, 1.1423283987539017, 1.143910855948113, 1.1433790077630441, 1.1445188506097819, 1.14630387849314, 1.1457940346538533, 1.146628463950404, 1.1473937797936171, 1.1481659045336357, 1.148434581808563, 1.1487810780631749, 1.149355193574682, 1.1488680560192555, 1.1497135695059877, 1.1494006097154332, 1.1497616761386882, 1.150554156108513, 1.1511218151539484, 1.1533774699437196, 1.1546042608630105, 1.1542671201014714, 1.1534484114893775, 1.1558203671218914, 1.1563410089841006, 1.1545763899260062, 1.1562686424489241, 1.1560987867516457]\n",
      "args {'num_dense2': 121}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.4316 - acc: 0.2363 - val_loss: 1.2137 - val_acc: 0.3212\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3159 - acc: 0.3041 - val_loss: 1.1449 - val_acc: 0.3716\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2731 - acc: 0.3412 - val_loss: 1.1246 - val_acc: 0.3924\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2479 - acc: 0.3636 - val_loss: 1.1199 - val_acc: 0.3988\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2361 - acc: 0.3718 - val_loss: 1.1183 - val_acc: 0.4007\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2179 - acc: 0.3771 - val_loss: 1.1181 - val_acc: 0.4007\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2084 - acc: 0.3821 - val_loss: 1.1180 - val_acc: 0.4017\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1991 - acc: 0.3879 - val_loss: 1.1187 - val_acc: 0.3975\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1859 - acc: 0.3829 - val_loss: 1.1194 - val_acc: 0.3973\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1801 - acc: 0.3923 - val_loss: 1.1222 - val_acc: 0.3937\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1681 - acc: 0.3959 - val_loss: 1.1240 - val_acc: 0.3922\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1548 - acc: 0.3972 - val_loss: 1.1244 - val_acc: 0.3907\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1457 - acc: 0.4058 - val_loss: 1.1252 - val_acc: 0.3922\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1412 - acc: 0.4074 - val_loss: 1.1274 - val_acc: 0.3908\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1310 - acc: 0.4138 - val_loss: 1.1290 - val_acc: 0.3868\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1290 - acc: 0.4174 - val_loss: 1.1298 - val_acc: 0.3833\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1282 - acc: 0.4172 - val_loss: 1.1309 - val_acc: 0.3798\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1128 - acc: 0.4224 - val_loss: 1.1343 - val_acc: 0.3711\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1140 - acc: 0.4180 - val_loss: 1.1338 - val_acc: 0.3718\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1052 - acc: 0.4268 - val_loss: 1.1344 - val_acc: 0.3709\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1080 - acc: 0.4194 - val_loss: 1.1366 - val_acc: 0.3667\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0949 - acc: 0.4323 - val_loss: 1.1373 - val_acc: 0.3672\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0892 - acc: 0.4358 - val_loss: 1.1394 - val_acc: 0.3656\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0881 - acc: 0.4315 - val_loss: 1.1391 - val_acc: 0.3665\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0805 - acc: 0.4375 - val_loss: 1.1402 - val_acc: 0.3643\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0788 - acc: 0.4415 - val_loss: 1.1417 - val_acc: 0.3604\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0768 - acc: 0.4422 - val_loss: 1.1439 - val_acc: 0.3568\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0721 - acc: 0.4455 - val_loss: 1.1442 - val_acc: 0.3564\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0626 - acc: 0.4518 - val_loss: 1.1454 - val_acc: 0.3569\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0599 - acc: 0.4511 - val_loss: 1.1457 - val_acc: 0.3580\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0593 - acc: 0.4480 - val_loss: 1.1471 - val_acc: 0.3534\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0602 - acc: 0.4453 - val_loss: 1.1488 - val_acc: 0.3549\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0522 - acc: 0.4584 - val_loss: 1.1491 - val_acc: 0.3512\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0511 - acc: 0.4558 - val_loss: 1.1500 - val_acc: 0.3506\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0474 - acc: 0.4627 - val_loss: 1.1505 - val_acc: 0.3484\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0452 - acc: 0.4602 - val_loss: 1.1525 - val_acc: 0.3467\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0433 - acc: 0.4635 - val_loss: 1.1524 - val_acc: 0.3469\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0430 - acc: 0.4590 - val_loss: 1.1521 - val_acc: 0.3506\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0357 - acc: 0.4690 - val_loss: 1.1533 - val_acc: 0.3484\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0328 - acc: 0.4684 - val_loss: 1.1560 - val_acc: 0.3449\n",
      "[1.213729773295348, 1.1448575956620053, 1.12463889784644, 1.1198633909225464, 1.1183164541987698, 1.1180651977211642, 1.1180462570866057, 1.1186964180553967, 1.1194219738651037, 1.1221570897167321, 1.124024776411966, 1.124357266387108, 1.1252066247794543, 1.1273946047478867, 1.1290372733524126, 1.1298336385056498, 1.1309266321015943, 1.1343219247760825, 1.1338403718672916, 1.1343648391458578, 1.1365725081363232, 1.137317098778665, 1.1393927212307173, 1.1391223844455438, 1.1401586935370756, 1.1416607965240686, 1.143934248578646, 1.1442166336225879, 1.1453583256750082, 1.1457354034974725, 1.1470533472965458, 1.1488386405586222, 1.1491488858204773, 1.149994969367981, 1.1504870688882771, 1.152461051291276, 1.1523900798620905, 1.1521173488216765, 1.1532674228787747, 1.1560297600255025]\n",
      "args {'num_dense2': 120}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 4s - loss: 1.2948 - acc: 0.3472 - val_loss: 1.1859 - val_acc: 0.3333\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.2834 - acc: 0.3432 - val_loss: 1.1825 - val_acc: 0.3287\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2504 - acc: 0.3501 - val_loss: 1.1760 - val_acc: 0.3265\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2365 - acc: 0.3568 - val_loss: 1.1731 - val_acc: 0.3210\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2116 - acc: 0.3614 - val_loss: 1.1719 - val_acc: 0.3174\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.1990 - acc: 0.3684 - val_loss: 1.1669 - val_acc: 0.3224\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.1954 - acc: 0.3690 - val_loss: 1.1669 - val_acc: 0.3166\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.1759 - acc: 0.3758 - val_loss: 1.1641 - val_acc: 0.3169\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1620 - acc: 0.3782 - val_loss: 1.1638 - val_acc: 0.3144\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1472 - acc: 0.3878 - val_loss: 1.1633 - val_acc: 0.3093\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1438 - acc: 0.3956 - val_loss: 1.1619 - val_acc: 0.3084\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1379 - acc: 0.3926 - val_loss: 1.1603 - val_acc: 0.3134\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1320 - acc: 0.4052 - val_loss: 1.1599 - val_acc: 0.3093\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1197 - acc: 0.4019 - val_loss: 1.1599 - val_acc: 0.3082\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1144 - acc: 0.4155 - val_loss: 1.1582 - val_acc: 0.3110\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1130 - acc: 0.4122 - val_loss: 1.1588 - val_acc: 0.3106\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1057 - acc: 0.4104 - val_loss: 1.1586 - val_acc: 0.3098\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.0966 - acc: 0.4231 - val_loss: 1.1583 - val_acc: 0.3113\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.0956 - acc: 0.4205 - val_loss: 1.1587 - val_acc: 0.3116\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.0863 - acc: 0.4237 - val_loss: 1.1577 - val_acc: 0.3149\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0829 - acc: 0.4302 - val_loss: 1.1578 - val_acc: 0.3156\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0826 - acc: 0.4282 - val_loss: 1.1573 - val_acc: 0.3137\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.0751 - acc: 0.4347 - val_loss: 1.1562 - val_acc: 0.3162\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0725 - acc: 0.4339 - val_loss: 1.1556 - val_acc: 0.3169\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0746 - acc: 0.4384 - val_loss: 1.1574 - val_acc: 0.3134\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0677 - acc: 0.4341 - val_loss: 1.1554 - val_acc: 0.3168\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0659 - acc: 0.4471 - val_loss: 1.1557 - val_acc: 0.3178\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0542 - acc: 0.4476 - val_loss: 1.1557 - val_acc: 0.3168\n",
      "Epoch 29/40\n",
      " - 5s - loss: 1.0538 - acc: 0.4479 - val_loss: 1.1561 - val_acc: 0.3188\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0482 - acc: 0.4582 - val_loss: 1.1555 - val_acc: 0.3215\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0500 - acc: 0.4529 - val_loss: 1.1563 - val_acc: 0.3215\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0490 - acc: 0.4528 - val_loss: 1.1564 - val_acc: 0.3186\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0448 - acc: 0.4528 - val_loss: 1.1581 - val_acc: 0.3181\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0405 - acc: 0.4613 - val_loss: 1.1568 - val_acc: 0.3210\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0370 - acc: 0.4656 - val_loss: 1.1561 - val_acc: 0.3214\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0377 - acc: 0.4619 - val_loss: 1.1573 - val_acc: 0.3222\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0329 - acc: 0.4557 - val_loss: 1.1585 - val_acc: 0.3251\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0270 - acc: 0.4646 - val_loss: 1.1578 - val_acc: 0.3236\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0306 - acc: 0.4693 - val_loss: 1.1580 - val_acc: 0.3254\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0271 - acc: 0.4698 - val_loss: 1.1596 - val_acc: 0.3236\n",
      "[1.1858922541953563, 1.182498676250676, 1.1759789230388258, 1.1730528936723923, 1.1719360838793929, 1.1669107984132272, 1.1668790705522334, 1.1641229696429718, 1.1637625658544597, 1.1633016722078868, 1.1618992895781182, 1.1603022573429491, 1.1598878169904288, 1.1599074608623494, 1.158179302306526, 1.1588289003606063, 1.1586312127048377, 1.1582848889301518, 1.1586596270672957, 1.15769164308865, 1.157761641679083, 1.1572748799414985, 1.1561523926030712, 1.1555945944721107, 1.1573893731556406, 1.1554274286171395, 1.155715714358504, 1.155682067455323, 1.156089536500562, 1.1554695286607872, 1.1563445274446575, 1.1563748322650587, 1.1580770681599506, 1.1568263119507876, 1.1561467007655213, 1.1573162488131823, 1.1584568277041984, 1.1577909177915278, 1.1580012919792362, 1.15963403567956]\n",
      "args {'num_dense2': 332}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 5s - loss: 1.4941 - acc: 0.3985 - val_loss: 1.1678 - val_acc: 0.3581\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3370 - acc: 0.3881 - val_loss: 1.1748 - val_acc: 0.3219\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.2821 - acc: 0.3676 - val_loss: 1.1796 - val_acc: 0.3050\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.2680 - acc: 0.3581 - val_loss: 1.1818 - val_acc: 0.2967\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2471 - acc: 0.3546 - val_loss: 1.1771 - val_acc: 0.2965\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2342 - acc: 0.3662 - val_loss: 1.1739 - val_acc: 0.2955\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2163 - acc: 0.3695 - val_loss: 1.1708 - val_acc: 0.2965\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.2062 - acc: 0.3765 - val_loss: 1.1697 - val_acc: 0.2973\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.1912 - acc: 0.3836 - val_loss: 1.1710 - val_acc: 0.2921\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.1792 - acc: 0.3842 - val_loss: 1.1644 - val_acc: 0.3013\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1610 - acc: 0.3960 - val_loss: 1.1649 - val_acc: 0.3007\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1665 - acc: 0.3932 - val_loss: 1.1630 - val_acc: 0.3067\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1457 - acc: 0.3994 - val_loss: 1.1636 - val_acc: 0.3038\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1446 - acc: 0.4077 - val_loss: 1.1621 - val_acc: 0.3076\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1333 - acc: 0.4066 - val_loss: 1.1596 - val_acc: 0.3099\n",
      "Epoch 16/40\n",
      " - 4s - loss: 1.1290 - acc: 0.4125 - val_loss: 1.1611 - val_acc: 0.3099\n",
      "Epoch 17/40\n",
      " - 5s - loss: 1.1201 - acc: 0.4160 - val_loss: 1.1588 - val_acc: 0.3128\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1187 - acc: 0.4169 - val_loss: 1.1558 - val_acc: 0.3162\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1118 - acc: 0.4283 - val_loss: 1.1608 - val_acc: 0.3127\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1042 - acc: 0.4221 - val_loss: 1.1579 - val_acc: 0.3156\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.0955 - acc: 0.4293 - val_loss: 1.1591 - val_acc: 0.3190\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.0953 - acc: 0.4337 - val_loss: 1.1594 - val_acc: 0.3181\n",
      "Epoch 23/40\n",
      " - 5s - loss: 1.0871 - acc: 0.4323 - val_loss: 1.1587 - val_acc: 0.3183\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0812 - acc: 0.4393 - val_loss: 1.1602 - val_acc: 0.3162\n",
      "Epoch 25/40\n",
      " - 5s - loss: 1.0851 - acc: 0.4350 - val_loss: 1.1591 - val_acc: 0.3188\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0793 - acc: 0.4338 - val_loss: 1.1579 - val_acc: 0.3210\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0745 - acc: 0.4399 - val_loss: 1.1589 - val_acc: 0.3224\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0688 - acc: 0.4468 - val_loss: 1.1594 - val_acc: 0.3203\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0580 - acc: 0.4560 - val_loss: 1.1600 - val_acc: 0.3227\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0628 - acc: 0.4479 - val_loss: 1.1591 - val_acc: 0.3265\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0587 - acc: 0.4512 - val_loss: 1.1590 - val_acc: 0.3260\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0496 - acc: 0.4626 - val_loss: 1.1610 - val_acc: 0.3241\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0500 - acc: 0.4595 - val_loss: 1.1613 - val_acc: 0.3248\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0452 - acc: 0.4615 - val_loss: 1.1610 - val_acc: 0.3237\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0489 - acc: 0.4599 - val_loss: 1.1606 - val_acc: 0.3234\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0355 - acc: 0.4669 - val_loss: 1.1618 - val_acc: 0.3237\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0316 - acc: 0.4657 - val_loss: 1.1618 - val_acc: 0.3251\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0365 - acc: 0.4659 - val_loss: 1.1613 - val_acc: 0.3263\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0241 - acc: 0.4725 - val_loss: 1.1621 - val_acc: 0.3251\n",
      "Epoch 40/40\n",
      " - 5s - loss: 1.0240 - acc: 0.4728 - val_loss: 1.1605 - val_acc: 0.3251\n",
      "[1.1677727348473157, 1.1747964843092562, 1.1796461847237736, 1.1818232088063003, 1.1771153646853705, 1.1738657909128256, 1.1707957024795157, 1.1696800691880063, 1.1709536223060752, 1.1643869301928487, 1.1649335824176466, 1.1630288001951794, 1.1636096638619737, 1.162073917869651, 1.1595717821199172, 1.1610701629836162, 1.1588140680614545, 1.1557838965501708, 1.1608389765430212, 1.157880775285352, 1.1591304089133032, 1.159432428734179, 1.158722914856851, 1.1601647596593123, 1.15912926879176, 1.1579249259236724, 1.158883406940533, 1.1594049553780206, 1.1599803568556784, 1.1590612967592493, 1.1590169112753803, 1.161013481077771, 1.1613167400905806, 1.1609662306731015, 1.1605670676244377, 1.1617600222699325, 1.1618321269994212, 1.161307645103912, 1.1621387842890352, 1.1605236955819402]\n",
      "args {'num_dense2': 237}\n",
      "Class weights {0: 1.9705474705474706, 1: 0.6056443024494143, 2: 1.1885057471264369}\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/40\n",
      " - 5s - loss: 1.6065 - acc: 0.2109 - val_loss: 1.3075 - val_acc: 0.2778\n",
      "Epoch 2/40\n",
      " - 4s - loss: 1.3959 - acc: 0.2573 - val_loss: 1.1884 - val_acc: 0.3425\n",
      "Epoch 3/40\n",
      " - 4s - loss: 1.3292 - acc: 0.3063 - val_loss: 1.1493 - val_acc: 0.3917\n",
      "Epoch 4/40\n",
      " - 4s - loss: 1.3082 - acc: 0.3363 - val_loss: 1.1385 - val_acc: 0.4004\n",
      "Epoch 5/40\n",
      " - 4s - loss: 1.2872 - acc: 0.3424 - val_loss: 1.1346 - val_acc: 0.4024\n",
      "Epoch 6/40\n",
      " - 4s - loss: 1.2734 - acc: 0.3518 - val_loss: 1.1310 - val_acc: 0.4011\n",
      "Epoch 7/40\n",
      " - 4s - loss: 1.2531 - acc: 0.3556 - val_loss: 1.1278 - val_acc: 0.3980\n",
      "Epoch 8/40\n",
      " - 4s - loss: 1.2352 - acc: 0.3626 - val_loss: 1.1282 - val_acc: 0.3941\n",
      "Epoch 9/40\n",
      " - 4s - loss: 1.2187 - acc: 0.3679 - val_loss: 1.1260 - val_acc: 0.3931\n",
      "Epoch 10/40\n",
      " - 4s - loss: 1.2144 - acc: 0.3695 - val_loss: 1.1275 - val_acc: 0.3895\n",
      "Epoch 11/40\n",
      " - 4s - loss: 1.1913 - acc: 0.3769 - val_loss: 1.1278 - val_acc: 0.3886\n",
      "Epoch 12/40\n",
      " - 4s - loss: 1.1845 - acc: 0.3822 - val_loss: 1.1283 - val_acc: 0.3861\n",
      "Epoch 13/40\n",
      " - 4s - loss: 1.1772 - acc: 0.3834 - val_loss: 1.1299 - val_acc: 0.3811\n",
      "Epoch 14/40\n",
      " - 4s - loss: 1.1633 - acc: 0.3879 - val_loss: 1.1309 - val_acc: 0.3793\n",
      "Epoch 15/40\n",
      " - 4s - loss: 1.1609 - acc: 0.3891 - val_loss: 1.1321 - val_acc: 0.3731\n",
      "Epoch 16/40\n",
      " - 5s - loss: 1.1516 - acc: 0.3925 - val_loss: 1.1328 - val_acc: 0.3697\n",
      "Epoch 17/40\n",
      " - 4s - loss: 1.1442 - acc: 0.3976 - val_loss: 1.1337 - val_acc: 0.3667\n",
      "Epoch 18/40\n",
      " - 4s - loss: 1.1314 - acc: 0.3967 - val_loss: 1.1338 - val_acc: 0.3660\n",
      "Epoch 19/40\n",
      " - 4s - loss: 1.1308 - acc: 0.4058 - val_loss: 1.1344 - val_acc: 0.3632\n",
      "Epoch 20/40\n",
      " - 4s - loss: 1.1231 - acc: 0.4068 - val_loss: 1.1361 - val_acc: 0.3610\n",
      "Epoch 21/40\n",
      " - 4s - loss: 1.1159 - acc: 0.4077 - val_loss: 1.1376 - val_acc: 0.3595\n",
      "Epoch 22/40\n",
      " - 4s - loss: 1.1110 - acc: 0.4167 - val_loss: 1.1396 - val_acc: 0.3541\n",
      "Epoch 23/40\n",
      " - 4s - loss: 1.1044 - acc: 0.4147 - val_loss: 1.1389 - val_acc: 0.3537\n",
      "Epoch 24/40\n",
      " - 4s - loss: 1.0983 - acc: 0.4221 - val_loss: 1.1415 - val_acc: 0.3498\n",
      "Epoch 25/40\n",
      " - 4s - loss: 1.0986 - acc: 0.4188 - val_loss: 1.1416 - val_acc: 0.3498\n",
      "Epoch 26/40\n",
      " - 4s - loss: 1.0910 - acc: 0.4271 - val_loss: 1.1429 - val_acc: 0.3471\n",
      "Epoch 27/40\n",
      " - 4s - loss: 1.0848 - acc: 0.4273 - val_loss: 1.1434 - val_acc: 0.3466\n",
      "Epoch 28/40\n",
      " - 4s - loss: 1.0809 - acc: 0.4282 - val_loss: 1.1432 - val_acc: 0.3469\n",
      "Epoch 29/40\n",
      " - 4s - loss: 1.0724 - acc: 0.4371 - val_loss: 1.1435 - val_acc: 0.3452\n",
      "Epoch 30/40\n",
      " - 4s - loss: 1.0699 - acc: 0.4351 - val_loss: 1.1446 - val_acc: 0.3438\n",
      "Epoch 31/40\n",
      " - 4s - loss: 1.0691 - acc: 0.4409 - val_loss: 1.1446 - val_acc: 0.3438\n",
      "Epoch 32/40\n",
      " - 4s - loss: 1.0666 - acc: 0.4392 - val_loss: 1.1465 - val_acc: 0.3425\n",
      "Epoch 33/40\n",
      " - 4s - loss: 1.0653 - acc: 0.4377 - val_loss: 1.1485 - val_acc: 0.3408\n",
      "Epoch 34/40\n",
      " - 4s - loss: 1.0554 - acc: 0.4468 - val_loss: 1.1475 - val_acc: 0.3391\n",
      "Epoch 35/40\n",
      " - 4s - loss: 1.0551 - acc: 0.4468 - val_loss: 1.1472 - val_acc: 0.3411\n",
      "Epoch 36/40\n",
      " - 4s - loss: 1.0548 - acc: 0.4509 - val_loss: 1.1495 - val_acc: 0.3353\n",
      "Epoch 37/40\n",
      " - 4s - loss: 1.0532 - acc: 0.4484 - val_loss: 1.1526 - val_acc: 0.3328\n",
      "Epoch 38/40\n",
      " - 4s - loss: 1.0452 - acc: 0.4481 - val_loss: 1.1514 - val_acc: 0.3340\n",
      "Epoch 39/40\n",
      " - 4s - loss: 1.0407 - acc: 0.4494 - val_loss: 1.1511 - val_acc: 0.3348\n",
      "Epoch 40/40\n",
      " - 4s - loss: 1.0358 - acc: 0.4626 - val_loss: 1.1531 - val_acc: 0.3312\n",
      "[1.3074930623701548, 1.1883898827295538, 1.1493378591797332, 1.138489117739311, 1.1346075749202384, 1.1310053180284005, 1.1278321275269303, 1.1282387747751594, 1.1260415097998013, 1.1275284667106025, 1.127755381430852, 1.1282637781927956, 1.129946567057264, 1.130898083263262, 1.1321176683545437, 1.1327617350326247, 1.1337440871412812, 1.1337816270235774, 1.1344258684553308, 1.136075441778843, 1.1375655751137383, 1.1396173086088426, 1.1388741682920533, 1.141451085620745, 1.1415956205503168, 1.1428925766931892, 1.1434494815665306, 1.143211405023892, 1.1434742147331343, 1.1445528989916602, 1.1446102022799873, 1.1464512910115296, 1.14852166078396, 1.147489131633852, 1.1472105083413604, 1.149474851117147, 1.1526400747351166, 1.1513778822298595, 1.1510961107726967, 1.1530857141401203]\n",
      "best: {'num_dense2': 23}\n"
     ]
    }
   ],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            trials=trials,\n",
    "            max_evals=20)\n",
    "print('best:', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "it=0\n",
    "loss = np.zeros(len(trials))\n",
    "params = np.zeros((len(trials),3))\n",
    "for objects in trials.trials:\n",
    "                  loss[it] = objects['result']['loss'] \n",
    "#                   params[it,0] = 10 ** objects['misc']['vals']['lr'][0]\n",
    "#                   params[it,1] = objects['misc']['vals']['num_dense1'][0]\n",
    "                  params[it,2] = objects['misc']['vals']['num_dense2'][0]\n",
    "                  it+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJPCAYAAAC+fJpMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzAH8oU/JrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6dc5b90b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(40 + params[:,2],loss[:])\n",
    "# plt.scatter(params[:,0][mask]+200,params[:,2][mask]+20,c=loss[:][mask],s=loss[:][mask]*200)\n",
    "# plt.yscale('log')\n",
    "# plt.ylim([1,2])\n",
    "# plt.xscale('log',nonposy='clip')\n",
    "# plt.xlim([10**-8,10**-3])\n",
    "# plt.colorbar()\n",
    "plt.xticks(range(0,400,20))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = loss>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.143590560401168, 'status': 'ok'},\n",
       " {'loss': 1.1558702121963293, 'status': 'ok'},\n",
       " {'loss': 1.1635656187904628, 'status': 'ok'},\n",
       " {'loss': 1.126508028045987, 'status': 'ok'},\n",
       " {'loss': 1.1630019833021659, 'status': 'ok'},\n",
       " {'loss': 1.139373833866795, 'status': 'ok'},\n",
       " {'loss': 1.1361741551911149, 'status': 'ok'},\n",
       " {'loss': 1.1571909660214623, 'status': 'ok'},\n",
       " {'loss': 1.164590459428626, 'status': 'ok'},\n",
       " {'loss': 1.1478215797070912, 'status': 'ok'},\n",
       " {'loss': 1.1394096999467231, 'status': 'ok'},\n",
       " {'loss': 1.116140023888944, 'status': 'ok'},\n",
       " {'loss': 1.1707677617060066, 'status': 'ok'},\n",
       " {'loss': 1.145261800256672, 'status': 'ok'},\n",
       " {'loss': 1.1377046822851944, 'status': 'ok'},\n",
       " {'loss': 1.1560987867516457, 'status': 'ok'},\n",
       " {'loss': 1.1560297600255025, 'status': 'ok'},\n",
       " {'loss': 1.15963403567956, 'status': 'ok'},\n",
       " {'loss': 1.1605236955819402, 'status': 'ok'},\n",
       " {'loss': 1.1530857141401203, 'status': 'ok'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials.results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
