{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17061, 2500, 1)\n",
      "(17061, 1)\n",
      "(5872, 2500, 1)\n",
      "(5872, 1)\n",
      "Train on 17061 samples, validate on 5872 samples\n",
      "Epoch 1/100\n",
      " - 16s - loss: 1.5462 - acc: 0.3087 - val_loss: 1.1749 - val_acc: 0.3595\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cdf138ee62f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m                          \u001b[0mlog_UAR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                          tensbd, csv_logger],\n\u001b[0;32m--> 138\u001b[0;31m               validation_data=(x_val,y_val))\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m###### Results #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1224\u001b[0m                             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m                                 \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/callbacks.pyc\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-cdf138ee62f9>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;31m# ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mtemp_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from heartnet_v1 import heartnet, reshape_folds\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "set_session(tf.Session(config=config))\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(1)\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, TensorBoard, Callback\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import os\n",
    "import tables\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "\n",
    "def heartnet_transfer(load_path='/media/taufiq/Data/heart_sound/interspeech_compare/weights.0148-0.8902.hdf5',lr=0.0012843784,lr_decay=0.0001132885,num_dense=20,trainable=False,dropout_rate=0.):\n",
    "    model = heartnet(load_path=load_path,FIR_train=False,trainable=trainable)\n",
    "    x = model.layers[-4].output\n",
    "    x = Dense(num_dense,activation='relu') (x)\n",
    "    x = Dropout(rate=dropout_rate,seed=1) (x)\n",
    "    output = Dense(3,activation='softmax')(x)\n",
    "    model = Model(inputs=model.input,outputs=output)\n",
    "    sgd = SGD(lr=lr)\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "class log_UAR(Callback):\n",
    "\n",
    "    def __init__(self, x_val, y_val, val_parts, res_thresh):\n",
    "        self.x_val = x_val\n",
    "        self.y_val = y_val\n",
    "        self.val_parts = val_parts\n",
    "        self.res_thresh = res_thresh\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if logs is not None:\n",
    "            y_pred = self.model.predict(self.x_val, verbose=0)\n",
    "            y_pred = np.argmax(y_pred, axis=-1)\n",
    "            self.y_val_ = np.transpose(np.argmax(self.y_val, axis=-1))\n",
    "            true = []\n",
    "            pred = []\n",
    "            start_idx = 0\n",
    "            for s in self.val_parts:\n",
    "\n",
    "                if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "                    continue\n",
    "                # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "\n",
    "                temp_ = self.y_val_[start_idx:start_idx + int(s) - 1]\n",
    "                temp = y_pred[start_idx:start_idx + int(s) - 1]\n",
    "\n",
    "                if (sum(temp == 0) > sum(temp == 1)) and (sum(temp == 0) > sum(temp == 2)):\n",
    "                    pred.append(0)\n",
    "                elif (sum(temp == 2) > sum(temp == 1)) and (sum(temp == 2) > sum(temp == 0)):\n",
    "                    pred.append(2)\n",
    "                else:\n",
    "                    pred.append(1)\n",
    "\n",
    "                if (sum(temp_ == 0) > sum(temp_ == 1)) and (sum(temp_ == 0) > sum(temp_ == 2)):\n",
    "                    true.append(0)\n",
    "                elif (sum(temp_ == 2) > sum(temp_ == 1)) and (sum(temp_ == 2) > sum(temp_ == 0)):\n",
    "                    true.append(2)\n",
    "                else:\n",
    "                    true.append(1)\n",
    "\n",
    "                start_idx = start_idx + int(s)\n",
    "\n",
    "            score = recall_score(y_pred=pred, y_true=true, average='macro')\n",
    "            logs['UAR'] = np.array(score)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    fold_dir = '/media/taufiq/Data/heart_sound/interspeech_compare/feature/segmented_noFIR/'\n",
    "    foldname = 'comParE'\n",
    "    model_dir = '/media/taufiq/Data/heart_sound/models/'\n",
    "    log_name = foldname + ' ' + str(datetime.now())\n",
    "    checkpoint_name = model_dir + log_name + \"/\" + 'weights.{epoch:04d}-{val_acc:.4f}.hdf5'\n",
    "    if not os.path.exists(model_dir + log_name):\n",
    "        os.makedirs(model_dir + log_name)\n",
    "    log_dir = '/media/taufiq/Data/heart_sound/interspeech_compare/logs/'\n",
    "\n",
    "    ##### Load Model ######\n",
    "\n",
    "    load_path='/media/taufiq/Data/heart_sound/interspeech_compare/weights.0148-0.8902.hdf5'\n",
    "    lr = 0.00001\n",
    "    num_dense = 34\n",
    "    epochs = 1\n",
    "    batch_size = 128\n",
    "    dropout_rate = 0.\n",
    "    trainable = True\n",
    "    res_thresh = .5\n",
    "    model = heartnet_transfer(load_path=load_path,lr=lr,num_dense=num_dense,trainable=trainable,dropout_rate=dropout_rate)\n",
    "    plot_model(model,\"model.png\",show_layer_names=True,show_shapes=True)\n",
    "\n",
    "    ###### Load Data ######\n",
    "\n",
    "    feat = tables.open_file(fold_dir + foldname + '.mat')\n",
    "    x_train = feat.root.trainX[:]\n",
    "    y_train = feat.root.trainY[0, :]\n",
    "    x_val = feat.root.valX[:]\n",
    "    y_val = feat.root.valY[0, :]\n",
    "    train_parts = feat.root.train_parts[:]\n",
    "    val_parts = feat.root.val_parts[0, :]\n",
    "\n",
    "    ############### Reshaping ############\n",
    "\n",
    "    x_train, y_train, x_val, y_val = reshape_folds(x_train, x_val, y_train, y_val)\n",
    "    y_train = to_categorical(y_train,num_classes=3)\n",
    "    y_val = to_categorical(y_val,num_classes=3)\n",
    "\n",
    "    ### Callbacks ###\n",
    "    csv_logger = CSVLogger(log_dir + log_name + '/training.csv')\n",
    "    modelcheckpnt = ModelCheckpoint(filepath=checkpoint_name,\n",
    "                                    monitor='val_acc', save_best_only=True, mode='max')\n",
    "    tensbd = TensorBoard(log_dir=log_dir + log_name,\n",
    "                         batch_size=batch_size,\n",
    "                         histogram_freq=100,\n",
    "                         # embeddings_freq=99,\n",
    "                         # embeddings_layer_names=embedding_layer_names,\n",
    "                         # embeddings_data=x_val,\n",
    "                         # embeddings_metadata=metadata_file,\n",
    "                         write_images=False)\n",
    "\n",
    "    ### Train ###\n",
    "    model.fit(x_train,y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=2,\n",
    "              shuffle=True,\n",
    "              callbacks=[modelcheckpnt,\n",
    "                         log_UAR(x_val, y_val, val_parts, res_thresh),\n",
    "                         tensbd, csv_logger],\n",
    "              validation_data=(x_val,y_val))\n",
    "\n",
    "    ###### Results #####\n",
    "\n",
    "    y_pred = model.predict(x_val, verbose=0)\n",
    "    y_pred = np.argmax(y_pred,axis=-1)\n",
    "    y_val = np.transpose(np.argmax(y_val,axis=-1))\n",
    "    true = []\n",
    "    pred = []\n",
    "    start_idx = 0\n",
    "    for s in val_parts:\n",
    "\n",
    "        if not s:  ## for e00032 in validation0 there was no cardiac cycle\n",
    "            continue\n",
    "        # ~ print \"part {} start {} stop {}\".format(s,start_idx,start_idx+int(s)-1)\n",
    "\n",
    "        temp_ = y_val[start_idx:start_idx + int(s) - 1]\n",
    "        temp = y_pred[start_idx:start_idx + int(s) - 1]\n",
    "\n",
    "        if (sum(temp==0) > sum(temp==1)) and  (sum(temp==0) > sum(temp==2)):\n",
    "            pred.append(0)\n",
    "        elif (sum(temp==2) > sum(temp==1)) and  (sum(temp==2) > sum(temp==0)):\n",
    "            pred.append(2)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "\n",
    "        if (sum(temp_ == 0) > sum(temp_ == 1)) and (sum(temp_ == 0) > sum(temp_ == 2)):\n",
    "            true.append(0)\n",
    "        elif (sum(temp_ == 2) > sum(temp_ == 1)) and (sum(temp_ == 2) > sum(temp_ == 0)):\n",
    "            true.append(2)\n",
    "        else:\n",
    "            true.append(1)\n",
    "\n",
    "        start_idx = start_idx + int(s)\n",
    "    score = recall_score(y_pred=pred, y_true=true, average='macro')\n",
    "    print(score)\n",
    "    print(confusion_matrix(y_true=true,y_pred=pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    y_pred = model.predict(x_val, verbose=0)\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(y_pred[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.argmax(y_pred)\n",
    "sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
